"""
Vision Transformer (ViT) å¿«é€Ÿå…¥é—¨
æ¼”ç¤º ViT çš„åŸºæœ¬åŠŸèƒ½å’Œä½¿ç”¨æ–¹æ³•
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import sys
import os

# ç¡®ä¿èƒ½å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from vit_components import test_components, demonstrate_attention_mechanism
from patch_embedding import demonstrate_embedding_process, analyze_patch_patterns
from vit_model import create_vit_model, count_parameters, test_vit_models

def quick_demo():
    """å¿«é€Ÿæ¼”ç¤º ViT çš„æ ¸å¿ƒåŠŸèƒ½"""
    print("ğŸš€ Vision Transformer (ViT) å¿«é€Ÿå…¥é—¨")
    print("=" * 60)
    
    # 1. åˆ›å»ºä¸€ä¸ªç®€å•çš„ ViT æ¨¡å‹
    print("\n1. åˆ›å»º ViT æ¨¡å‹")
    print("-" * 30)
    
    model = create_vit_model(
        model_name='vit_tiny',
        img_size=224,
        patch_size=16,
        num_classes=10
    )
    
    total_params, trainable_params = count_parameters(model)
    print(f"âœ“ åˆ›å»ºäº† ViT-Tiny æ¨¡å‹")
    print(f"  - å›¾åƒå°ºå¯¸: 224x224")
    print(f"  - Patch å¤§å°: 16x16")
    print(f"  - åˆ†ç±»ç±»åˆ«: 10")
    print(f"  - å‚æ•°æ•°é‡: {total_params:,}")
    
    # 2. åˆ›å»ºéšæœºè¾“å…¥å›¾åƒ
    print("\n2. åˆ›å»ºè¾“å…¥å›¾åƒ")
    print("-" * 30)
    
    batch_size = 2
    channels = 3
    height = width = 224
    
    # åˆ›å»ºéšæœºå›¾åƒ
    images = torch.randn(batch_size, channels, height, width)
    print(f"âœ“ åˆ›å»ºäº†éšæœºå›¾åƒ: {images.shape}")
    
    # 3. æ¨¡å‹æ¨ç†
    print("\n3. æ¨¡å‹æ¨ç†")
    print("-" * 30)
    
    model.eval()
    with torch.no_grad():
        # åŸºæœ¬æ¨ç†
        logits = model(images)
        probabilities = torch.softmax(logits, dim=-1)
        predictions = torch.argmax(probabilities, dim=-1)
        
        print(f"âœ“ æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {logits.shape}")
        print(f"âœ“ é¢„æµ‹ç»“æœ: {predictions.tolist()}")
        print(f"âœ“ æœ€é«˜æ¦‚ç‡: {probabilities.max(dim=-1)[0].tolist()}")
        
        # è·å–æ³¨æ„åŠ›æƒé‡
        logits_with_attn, attention_weights = model(images, return_attention=True)
        print(f"âœ“ æ³¨æ„åŠ›å±‚æ•°: {len(attention_weights)}")
        print(f"âœ“ æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights[0].shape}")
    
    # 4. åˆ†æå›¾åƒåˆ†å—
    print("\n4. å›¾åƒåˆ†å—åˆ†æ")
    print("-" * 30)
    
    patch_size = 16
    n_patches = (224 // patch_size) ** 2
    print(f"âœ“ Patch å¤§å°: {patch_size}x{patch_size}")
    print(f"âœ“ Patch æ•°é‡: {n_patches}")
    print(f"âœ“ åºåˆ—é•¿åº¦: {n_patches + 1} (åŒ…å« CLS token)")
    
    # åˆ†æç¬¬ä¸€å¼ å›¾åƒçš„ patch embeddings
    patch_embeddings = model.patch_embed(images[:1])
    cls_token = patch_embeddings[:, 0, :]  # CLS token
    patch_tokens = patch_embeddings[:, 1:, :]  # Patch tokens
    
    print(f"âœ“ CLS token å½¢çŠ¶: {cls_token.shape}")
    print(f"âœ“ Patch tokens å½¢çŠ¶: {patch_tokens.shape}")
    
    # 5. æ³¨æ„åŠ›åˆ†æ
    print("\n5. æ³¨æ„åŠ›åˆ†æ")
    print("-" * 30)
    
    # è·å–æœ€åä¸€å±‚çš„æ³¨æ„åŠ›
    last_layer_attention = attention_weights[-1]  # [batch, heads, seq_len, seq_len]
    
    # CLS token å¯¹å…¶ä»– token çš„æ³¨æ„åŠ›
    cls_attention = last_layer_attention[0, :, 0, 1:]  # [heads, n_patches]
    
    print(f"âœ“ æœ€åä¸€å±‚æ³¨æ„åŠ›å½¢çŠ¶: {last_layer_attention.shape}")
    print(f"âœ“ CLS token æ³¨æ„åŠ›ç»Ÿè®¡:")
    print(f"  - æœ€å¤§å€¼: {cls_attention.max().item():.4f}")
    print(f"  - æœ€å°å€¼: {cls_attention.min().item():.4f}")
    print(f"  - å¹³å‡å€¼: {cls_attention.mean().item():.4f}")
    
    # æ‰¾å‡º CLS token æœ€å…³æ³¨çš„ patches
    avg_cls_attention = cls_attention.mean(dim=0)  # å¹³å‡æ‰€æœ‰å¤´
    top_patches = torch.topk(avg_cls_attention, 5)
    
    print(f"âœ“ CLS token æœ€å…³æ³¨çš„ 5 ä¸ª patches:")
    for i, (score, patch_idx) in enumerate(zip(top_patches.values, top_patches.indices)):
        row = patch_idx // 14  # 14x14 patches
        col = patch_idx % 14
        print(f"  {i+1}. Patch ({row}, {col}): æ³¨æ„åŠ› = {score:.4f}")
    
    # 6. æ¨¡å‹å˜ä½“æ¯”è¾ƒ
    print("\n6. æ¨¡å‹å˜ä½“æ¯”è¾ƒ")
    print("-" * 30)
    
    model_variants = ['vit_tiny', 'vit_small', 'vit_base']
    
    print(f"{'æ¨¡å‹':<12} {'åµŒå…¥ç»´åº¦':<8} {'å±‚æ•°':<6} {'å¤´æ•°':<6} {'å‚æ•°æ•°é‡':<12}")
    print("-" * 50)
    
    for variant in model_variants:
        try:
            temp_model = create_vit_model(model_name=variant, num_classes=10)
            params, _ = count_parameters(temp_model)
            
            embed_dim = temp_model.embed_dim
            n_layers = temp_model.n_layers
            n_heads = temp_model.transformer_blocks[0].attention.n_heads
            
            print(f"{variant:<12} {embed_dim:<8} {n_layers:<6} {n_heads:<6} {params:<12,}")
            
        except Exception as e:
            print(f"{variant:<12} åˆ›å»ºå¤±è´¥: {e}")
    
    print("\nâœ… ViT å¿«é€Ÿå…¥é—¨æ¼”ç¤ºå®Œæˆï¼")
    return model, images, attention_weights


def visualize_attention_pattern(model, images, attention_weights, save_plots=False):
    """å¯è§†åŒ–æ³¨æ„åŠ›æ¨¡å¼"""
    print("\n" + "=" * 60)
    print("æ³¨æ„åŠ›æ¨¡å¼å¯è§†åŒ–")
    print("=" * 60)
    
    # ä½¿ç”¨ç¬¬ä¸€å¼ å›¾åƒ
    img_idx = 0
    
    # è·å–ä¸åŒå±‚çš„æ³¨æ„åŠ›
    n_layers = len(attention_weights)
    layers_to_show = [0, n_layers//2, n_layers-1]  # ç¬¬ä¸€å±‚ã€ä¸­é—´å±‚ã€æœ€åä¸€å±‚
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    for i, layer_idx in enumerate(layers_to_show):
        # CLS token çš„æ³¨æ„åŠ›
        cls_attn = attention_weights[layer_idx][img_idx, :, 0, 1:]  # [heads, n_patches]
        cls_attn_avg = cls_attn.mean(dim=0).detach().numpy()  # å¹³å‡æ‰€æœ‰å¤´
        
        # é‡å¡‘ä¸º2Dç½‘æ ¼ (14x14)
        grid_size = int(np.sqrt(len(cls_attn_avg)))
        attn_grid = cls_attn_avg.reshape(grid_size, grid_size)
        
        # ç»˜åˆ¶æ³¨æ„åŠ›çƒ­åŠ›å›¾
        im1 = axes[0, i].imshow(attn_grid, cmap='hot', interpolation='nearest')
        axes[0, i].set_title(f'ç¬¬ {layer_idx+1} å±‚ CLS æ³¨æ„åŠ›')
        axes[0, i].axis('off')
        plt.colorbar(im1, ax=axes[0, i], fraction=0.046, pad=0.04)
        
        # å¹³å‡æ³¨æ„åŠ›ï¼ˆæ‰€æœ‰tokenå¯¹æ‰€æœ‰tokenï¼‰
        avg_attn = attention_weights[layer_idx][img_idx].mean(dim=0)[1:, 1:].detach().numpy()
        
        # æ˜¾ç¤ºéƒ¨åˆ†æ³¨æ„åŠ›çŸ©é˜µ
        sample_size = min(49, avg_attn.shape[0])  # æ˜¾ç¤º7x7çš„å­çŸ©é˜µ
        sample_attn = avg_attn[:sample_size, :sample_size]
        
        im2 = axes[1, i].imshow(sample_attn, cmap='viridis', interpolation='nearest')
        axes[1, i].set_title(f'ç¬¬ {layer_idx+1} å±‚ Patch é—´æ³¨æ„åŠ›')
        axes[1, i].axis('off')
        plt.colorbar(im2, ax=axes[1, i], fraction=0.046, pad=0.04)
    
    plt.tight_layout()
    
    if save_plots:
        plt.savefig('vit/attention_patterns.png', dpi=300, bbox_inches='tight')
        print("âœ“ æ³¨æ„åŠ›æ¨¡å¼å›¾å·²ä¿å­˜åˆ° vit/attention_patterns.png")
    
    try:
        plt.show()
    except:
        print("æ³¨æ„åŠ›å¯è§†åŒ–å®Œæˆï¼ˆæ— æ³•æ˜¾ç¤ºå›¾å½¢ç•Œé¢ï¼‰")
    
    return fig


def demonstrate_different_patch_sizes():
    """æ¼”ç¤ºä¸åŒ patch å¤§å°çš„å½±å“"""
    print("\n" + "=" * 60)
    print("ä¸åŒ Patch å¤§å°çš„å½±å“")
    print("=" * 60)
    
    img_size = 224
    patch_sizes = [8, 16, 32]
    test_image = torch.randn(1, 3, img_size, img_size)
    
    print(f"{'Patchå¤§å°':<10} {'Patchæ•°é‡':<10} {'åºåˆ—é•¿åº¦':<10} {'è®¡ç®—å¤æ‚åº¦':<12}")
    print("-" * 50)
    
    for patch_size in patch_sizes:
        n_patches = (img_size // patch_size) ** 2
        seq_len = n_patches + 1  # +1 for CLS token
        
        # ä¼°ç®—æ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦ (O(nÂ²))
        attention_ops = seq_len ** 2
        
        # åˆ›å»ºæ¨¡å‹æµ‹è¯•
        try:
            model = create_vit_model(
                model_name='vit_tiny',
                patch_size=patch_size,
                img_size=img_size,
                num_classes=10
            )
            
            # æµ‹è¯•æ¨ç†æ—¶é—´
            import time
            model.eval()
            
            start_time = time.time()
            with torch.no_grad():
                for _ in range(10):  # å¤šæ¬¡æµ‹è¯•å–å¹³å‡
                    _ = model(test_image)
            avg_time = (time.time() - start_time) / 10
            
            print(f"{patch_size:<10} {n_patches:<10} {seq_len:<10} {attention_ops:<12,} ({avg_time:.3f}s)")
            
        except Exception as e:
            print(f"{patch_size:<10} æµ‹è¯•å¤±è´¥: {e}")
    
    print("\nè§‚å¯Ÿ:")
    print("- æ›´å°çš„ patch æä¾›æ›´ç»†ç²’åº¦çš„ä¿¡æ¯ï¼Œä½†è®¡ç®—é‡æ›´å¤§")
    print("- æ›´å¤§çš„ patch è®¡ç®—æ•ˆç‡æ›´é«˜ï¼Œä½†å¯èƒ½ä¸¢å¤±ç»†èŠ‚")
    print("- patch_size=16 æ˜¯å¸¸ç”¨çš„å¹³è¡¡é€‰æ‹©")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„å¿«é€Ÿå…¥é—¨æ¼”ç¤º"""
    try:
        # åŸºæœ¬æ¼”ç¤º
        model, images, attention_weights = quick_demo()
        
        # å¯è§†åŒ–æ³¨æ„åŠ›ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            visualize_attention_pattern(model, images, attention_weights, save_plots=True)
        except Exception as e:
            print(f"æ³¨æ„åŠ›å¯è§†åŒ–è·³è¿‡: {e}")
        
        # æ¼”ç¤ºä¸åŒ patch å¤§å°
        demonstrate_different_patch_sizes()
        
        print("\nğŸ‰ ViT å¿«é€Ÿå…¥é—¨å®Œæˆï¼")
        print("\nğŸ“š æ¥ä¸‹æ¥å¯ä»¥ï¼š")
        print("1. è¿è¡Œ vit_components.py æ·±å…¥äº†è§£å„ä¸ªç»„ä»¶")
        print("2. è¿è¡Œ patch_embedding.py å­¦ä¹ å›¾åƒåµŒå…¥è¿‡ç¨‹")
        print("3. è¿è¡Œ vit_model.py æµ‹è¯•å®Œæ•´æ¨¡å‹")
        print("4. è¿è¡Œ vit_trainer.py è¿›è¡Œå®é™…è®­ç»ƒ")
        print("5. é˜…è¯» README_vit_tutorial.md è·å–è¯¦ç»†æ•™ç¨‹")
        
    except Exception as e:
        print(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥ä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…")


if __name__ == "__main__":
    main()

"""
Transformer å¿«é€Ÿå…¥é—¨è„šæœ¬
ç”¨äºæµ‹è¯•åŸºæœ¬åŠŸèƒ½çš„ç®€å•è„šæœ¬
"""

import torch
import torch.nn as nn
import numpy as np

def test_basic_components():
    """æµ‹è¯•åŸºç¡€ç»„ä»¶"""
    print("=" * 50)
    print("æµ‹è¯• Transformer åŸºç¡€ç»„ä»¶")
    print("=" * 50)
    
    # æµ‹è¯•åŸºç¡€ç»„ä»¶æ˜¯å¦å¯ä»¥å¯¼å…¥å’Œè¿è¡Œ
    try:
        from transformer_components import MultiHeadAttention, PositionwiseFeedForward
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        seq_len = 10
        d_model = 64
        
        x = torch.randn(batch_size, seq_len, d_model)
        print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
        
        # æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›
        multi_head_attn = MultiHeadAttention(d_model, n_heads=4)
        attn_output, attn_weights = multi_head_attn(x, x, x)
        print(f"å¤šå¤´æ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {attn_output.shape}")
        print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attn_weights.shape}")
        
        # æµ‹è¯•å‰é¦ˆç½‘ç»œ
        ff = PositionwiseFeedForward(d_model, d_ff=256)
        ff_output = ff(x)
        print(f"å‰é¦ˆç½‘ç»œè¾“å‡ºå½¢çŠ¶: {ff_output.shape}")
        
        print("âœ“ åŸºç¡€ç»„ä»¶æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— åŸºç¡€ç»„ä»¶æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_positional_encoding():
    """æµ‹è¯•ä½ç½®ç¼–ç """
    print("\n" + "=" * 50)
    print("æµ‹è¯•ä½ç½®ç¼–ç ")
    print("=" * 50)
    
    try:
        from positional_encoding import PositionalEncoding
        
        # æµ‹è¯•ä½ç½®ç¼–ç 
        d_model = 64
        seq_len = 20
        batch_size = 2
        
        pos_encoding = PositionalEncoding(d_model, max_seq_len=100)
        x = torch.randn(batch_size, seq_len, d_model)
        
        x_with_pos = pos_encoding(x)
        print(f"åŸå§‹è¾“å…¥å½¢çŠ¶: {x.shape}")
        print(f"æ·»åŠ ä½ç½®ç¼–ç åå½¢çŠ¶: {x_with_pos.shape}")
        
        # éªŒè¯ä½ç½®ç¼–ç çš„ä½œç”¨
        print(f"ä½ç½®ç¼–ç å‰åæ˜¯å¦ç›¸åŒ: {torch.equal(x, x_with_pos)}")
        
        print("âœ“ ä½ç½®ç¼–ç æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— ä½ç½®ç¼–ç æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_transformer_model():
    """æµ‹è¯•å®Œæ•´æ¨¡å‹"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•å®Œæ•´ Transformer æ¨¡å‹")
    print("=" * 50)
    
    try:
        from transformer_model import Transformer, TransformerForLanguageModeling
        
        # æµ‹è¯•ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹
        src_vocab_size = 100
        tgt_vocab_size = 100
        model = Transformer(
            src_vocab_size, tgt_vocab_size,
            d_model=128, n_heads=4, n_layers=2, d_ff=256
        )
        
        batch_size = 2
        src_seq_len = 10
        tgt_seq_len = 8
        
        src = torch.randint(0, src_vocab_size, (batch_size, src_seq_len))
        tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_len))
        
        output, _, _, _ = model(src, tgt)
        print(f"ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æµ‹è¯•è¯­è¨€æ¨¡å‹
        lm_model = TransformerForLanguageModeling(
            vocab_size=100, d_model=128, n_heads=4, n_layers=2, d_ff=256
        )
        
        input_ids = torch.randint(0, 100, (batch_size, 15))
        logits, attention_weights = lm_model(input_ids)
        print(f"è¯­è¨€æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {logits.shape}")
        print(f"æ³¨æ„åŠ›å±‚æ•°: {len(attention_weights)}")
        
        print("âœ“ æ¨¡å‹æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def demo_simple_training():
    """æ¼”ç¤ºç®€å•è®­ç»ƒ"""
    print("\n" + "=" * 50)
    print("æ¼”ç¤ºç®€å•è®­ç»ƒè¿‡ç¨‹")
    print("=" * 50)
    
    try:
        from transformer_model import TransformerForLanguageModeling
        
        # åˆ›å»ºå°æ¨¡å‹
        model = TransformerForLanguageModeling(
            vocab_size=50, d_model=64, n_heads=4, n_layers=2, d_ff=128
        )
        
        # åˆ›å»ºéšæœºæ•°æ®
        batch_size = 4
        seq_len = 10
        data = torch.randint(0, 50, (batch_size, seq_len))
        targets = torch.randint(0, 50, (batch_size, seq_len))
        
        # è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()
        
        print("å¼€å§‹ç®€å•è®­ç»ƒæ¼”ç¤º...")
        
        # è®­ç»ƒå‡ æ­¥
        for step in range(5):
            optimizer.zero_grad()
            
            logits, _ = model(data)
            loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))
            
            loss.backward()
            optimizer.step()
            
            print(f"æ­¥éª¤ {step+1}, æŸå¤±: {loss.item():.4f}")
        
        print("âœ“ ç®€å•è®­ç»ƒæ¼”ç¤ºå®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âœ— è®­ç»ƒæ¼”ç¤ºå¤±è´¥: {e}")
        return False

def demo_text_generation():
    """æ¼”ç¤ºæ–‡æœ¬ç”Ÿæˆ"""
    print("\n" + "=" * 50)
    print("æ¼”ç¤ºæ–‡æœ¬ç”Ÿæˆ")
    print("=" * 50)
    
    try:
        from transformer_model import TransformerForLanguageModeling
        
        # åˆ›å»ºæ¨¡å‹
        vocab_size = 20
        model = TransformerForLanguageModeling(
            vocab_size=vocab_size, d_model=64, n_heads=4, n_layers=2, d_ff=128
        )
        
        # ç®€å•çš„ç”Ÿæˆç¤ºä¾‹
        model.eval()
        with torch.no_grad():
            # ä»éšæœºè¾“å…¥å¼€å§‹
            input_ids = torch.randint(0, vocab_size, (1, 5))
            print(f"åˆå§‹è¾“å…¥: {input_ids[0].tolist()}")
            
            # ç”Ÿæˆå‡ ä¸ªtoken
            generated = input_ids.clone()
            for i in range(10):
                logits, _ = model(generated)
                next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)
                generated = torch.cat([generated, next_token], dim=1)
            
            print(f"ç”Ÿæˆåºåˆ—: {generated[0].tolist()}")
        
        print("âœ“ æ–‡æœ¬ç”Ÿæˆæ¼”ç¤ºå®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âœ— æ–‡æœ¬ç”Ÿæˆæ¼”ç¤ºå¤±è´¥: {e}")
        return False

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹è¿è¡Œ Transformer å¿«é€Ÿæµ‹è¯•å¥—ä»¶...")
    
    tests = [
        ("åŸºç¡€ç»„ä»¶", test_basic_components),
        ("ä½ç½®ç¼–ç ", test_positional_encoding),
        ("å®Œæ•´æ¨¡å‹", test_transformer_model),
        ("ç®€å•è®­ç»ƒ", demo_simple_training),
        ("æ–‡æœ¬ç”Ÿæˆ", demo_text_generation)
    ]
    
    results = []
    
    for name, test_func in tests:
        success = test_func()
        results.append((name, success))
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 50)
    print("æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 50)
    
    passed = 0
    for name, success in results:
        status = "âœ“ é€šè¿‡" if success else "âœ— å¤±è´¥"
        print(f"{name}: {status}")
        if success:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{len(results)} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == len(results):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼æ‚¨çš„ Transformer å®ç°å·¥ä½œæ­£å¸¸ã€‚")
        print("\næ¥ä¸‹æ¥æ‚¨å¯ä»¥:")
        print("1. è¿è¡Œ train_transformer.py è¿›è¡Œå®Œæ•´è®­ç»ƒ")
        print("2. è¿è¡Œ attention_visualization.py æŸ¥çœ‹æ³¨æ„åŠ›å¯è§†åŒ–")
        print("3. è¿è¡Œ text_generation_example.py ä½“éªŒæ–‡æœ¬ç”Ÿæˆ")
    else:
        print(f"\nâš ï¸  æœ‰ {len(results) - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        print("è¯·æ£€æŸ¥ä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…ï¼š")
        print("pip install torch numpy")

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒ"""
    print("æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
    print(f"Python ç‰ˆæœ¬: {sys.version}")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"NumPy ç‰ˆæœ¬: {np.__version__}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDA è®¾å¤‡: {torch.cuda.get_device_name(0)}")
    
    print()

if __name__ == "__main__":
    import sys
    
    print("ğŸš€ Transformer å¿«é€Ÿå…¥é—¨æµ‹è¯•")
    print("è¿™ä¸ªè„šæœ¬å°†æµ‹è¯• Transformer å®ç°çš„åŸºæœ¬åŠŸèƒ½")
    print()
    
    # æ£€æŸ¥ç¯å¢ƒ
    check_environment()
    
    # è¿è¡Œæµ‹è¯•
    run_all_tests()

"""
BERT å¿«é€Ÿå…¥é—¨æ¼”ç¤º
å±•ç¤º BERT æ¨¡å‹çš„åŸºæœ¬ä½¿ç”¨å’ŒåŠŸèƒ½
"""

import torch
import torch.nn.functional as F
from bert_model import (
    BERTConfig, BERTModel, BERTForSequenceClassification, 
    BERTForTokenClassification, BERTForQuestionAnswering
)
from bert_pretraining import SimpleTokenizer

def quick_demo():
    """å¿«é€Ÿæ¼”ç¤º BERT çš„æ ¸å¿ƒåŠŸèƒ½"""
    print("ğŸ¤– BERT å¿«é€Ÿå…¥é—¨æ¼”ç¤º")
    print("=" * 60)
    
    print("ğŸ’¡ BERT (Bidirectional Encoder Representations from Transformers)")
    print("   æ˜¯ Google åœ¨ 2018 å¹´æå‡ºçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹")
    print("   é€šè¿‡åŒå‘ Transformer ç¼–ç å™¨å­¦ä¹ æ·±å±‚æ–‡æœ¬è¡¨ç¤º")
    print()
    
    # åˆ›å»ºå°å‹é…ç½®ç”¨äºæ¼”ç¤º
    config = BERTConfig(
        vocab_size=1000,
        hidden_size=256,
        num_hidden_layers=4,
        num_attention_heads=8,
        intermediate_size=512,
        max_position_embeddings=128
    )
    
    print(f"ğŸ“‹ æ¨¡å‹é…ç½®:")
    print(f"   è¯æ±‡è¡¨å¤§å°: {config.vocab_size:,}")
    print(f"   éšè—å±‚ç»´åº¦: {config.hidden_size}")
    print(f"   ç¼–ç å™¨å±‚æ•°: {config.num_hidden_layers}")
    print(f"   æ³¨æ„åŠ›å¤´æ•°: {config.num_attention_heads}")
    print(f"   å‰é¦ˆç½‘ç»œç»´åº¦: {config.intermediate_size}")
    
    # 1. åŸºç¡€ BERT æ¨¡å‹æ¼”ç¤º
    demonstrate_base_bert(config)
    
    # 2. æ–‡æœ¬åˆ†ç±»æ¼”ç¤º
    demonstrate_text_classification(config)
    
    # 3. è¯çº§åˆ†ç±»æ¼”ç¤º
    demonstrate_token_classification(config)
    
    # 4. é—®ç­”ç³»ç»Ÿæ¼”ç¤º
    demonstrate_question_answering(config)
    
    # 5. æ³¨æ„åŠ›å¯è§†åŒ–
    demonstrate_attention_visualization(config)
    
    print("\nğŸ‰ BERT å¿«é€Ÿå…¥é—¨æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)


def demonstrate_base_bert(config):
    """æ¼”ç¤ºåŸºç¡€ BERT æ¨¡å‹"""
    print("\nğŸ—ï¸ 1. åŸºç¡€ BERT æ¨¡å‹æ¼”ç¤º")
    print("-" * 40)
    
    # åˆ›å»ºæ¨¡å‹
    model = BERTModel(config)
    tokenizer = SimpleTokenizer()
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    sentence_a = "äººå·¥æ™ºèƒ½æ”¹å˜ä¸–ç•Œ"
    sentence_b = "æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£"
    
    print(f"è¾“å…¥å¥å­A: {sentence_a}")
    print(f"è¾“å…¥å¥å­B: {sentence_b}")
    
    # æ„é€  BERT è¾“å…¥
    tokens = ['[CLS]'] + sentence_a.split() + ['[SEP]'] + sentence_b.split() + ['[SEP]']
    input_ids = [tokenizer.get(token, 4) for token in tokens]
    token_type_ids = [0] * 6 + [1] * 5  # å¥å­Aä¸º0ï¼Œå¥å­Bä¸º1
    attention_mask = [1] * len(input_ids)
    
    # å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦
    max_len = 20
    input_ids.extend([3] * (max_len - len(input_ids)))  # ç”¨PADå¡«å……
    token_type_ids.extend([1] * (max_len - len(token_type_ids)))
    attention_mask.extend([0] * (max_len - len(attention_mask)))
    
    # è½¬æ¢ä¸ºå¼ é‡
    input_ids = torch.tensor([input_ids])
    token_type_ids = torch.tensor([token_type_ids])
    attention_mask = torch.tensor([attention_mask])
    
    print(f"\nè¾“å…¥å¤„ç†:")
    print(f"  è¾“å…¥tokens: {tokens}")
    print(f"  è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    print(f"  tokenç±»å‹: {token_type_ids[0][:len(tokens)]}")
    
    # å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        sequence_output, pooled_output = model(input_ids, attention_mask, token_type_ids)
    
    print(f"\næ¨¡å‹è¾“å‡º:")
    print(f"  åºåˆ—è¡¨ç¤ºå½¢çŠ¶: {sequence_output.shape}")
    print(f"  æ± åŒ–è¡¨ç¤ºå½¢çŠ¶: {pooled_output.shape}")
    print(f"  [CLS] tokenè¡¨ç¤º: {pooled_output[0][:5].tolist()}")  # åªæ˜¾ç¤ºå‰5ä¸ªå€¼
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"  æ¨¡å‹å‚æ•°æ€»é‡: {total_params:,}")


def demonstrate_text_classification(config):
    """æ¼”ç¤ºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡"""
    print("\nğŸ“ 2. æ–‡æœ¬åˆ†ç±»æ¼”ç¤º (æƒ…æ„Ÿåˆ†æ)")
    print("-" * 40)
    
    # åˆ›å»ºåˆ†ç±»æ¨¡å‹
    num_labels = 3  # æ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§
    model = BERTForSequenceClassification(config, num_labels=num_labels)
    tokenizer = SimpleTokenizer()
    
    # ç¤ºä¾‹æ–‡æœ¬
    texts = [
        "è¿™ä¸ªäº§å“çœŸçš„å¾ˆæ£’",
        "æœåŠ¡æ€åº¦å¤ªå·®äº†",
        "è¿˜è¡Œå§æ²¡ä»€ä¹ˆç‰¹åˆ«çš„"
    ]
    
    labels = ["æ­£é¢", "è´Ÿé¢", "ä¸­æ€§"]
    
    print("ç¤ºä¾‹æ–‡æœ¬åˆ†ç±»:")
    
    for i, text in enumerate(texts):
        # æ„é€ è¾“å…¥
        tokens = ['[CLS]'] + text.split() + ['[SEP]']
        input_ids = [tokenizer.get(token, 4) for token in tokens]
        
        # å¡«å……
        max_len = 15
        input_ids.extend([3] * (max_len - len(input_ids)))
        attention_mask = [1] * len(tokens) + [0] * (max_len - len(tokens))
        
        # è½¬æ¢ä¸ºå¼ é‡
        input_ids = torch.tensor([input_ids])
        attention_mask = torch.tensor([attention_mask])
        
        # å‰å‘ä¼ æ’­
        model.eval()
        with torch.no_grad():
            logits = model(input_ids, attention_mask)[0]
            probabilities = F.softmax(logits, dim=-1)
            predicted_label = torch.argmax(logits, dim=-1).item()
        
        print(f"  æ–‡æœ¬: '{text}'")
        print(f"    é¢„æµ‹ç±»åˆ«: {labels[predicted_label]}")
        print(f"    ç½®ä¿¡åº¦åˆ†å¸ƒ: {probabilities[0].tolist()}")
        print()


def demonstrate_token_classification(config):
    """æ¼”ç¤ºè¯çº§åˆ†ç±»ä»»åŠ¡"""
    print("\nğŸ·ï¸ 3. è¯çº§åˆ†ç±»æ¼”ç¤º (å‘½åå®ä½“è¯†åˆ«)")
    print("-" * 40)
    
    # åˆ›å»ºè¯çº§åˆ†ç±»æ¨¡å‹
    num_labels = 5  # O, B-PER, I-PER, B-ORG, I-ORG
    model = BERTForTokenClassification(config, num_labels=num_labels)
    tokenizer = SimpleTokenizer()
    
    # ç¤ºä¾‹æ–‡æœ¬
    text = "å¼ ä¸‰åœ¨åŒ—äº¬å¤§å­¦å·¥ä½œ"
    tokens = text.split()
    
    print(f"è¾“å…¥æ–‡æœ¬: {text}")
    print(f"åˆ†è¯ç»“æœ: {tokens}")
    
    # æ„é€ è¾“å…¥
    bert_tokens = ['[CLS]'] + tokens + ['[SEP]']
    input_ids = [tokenizer.get(token, 4) for token in bert_tokens]
    
    # å¡«å……
    max_len = 15
    input_ids.extend([3] * (max_len - len(input_ids)))
    attention_mask = [1] * len(bert_tokens) + [0] * (max_len - len(bert_tokens))
    
    # è½¬æ¢ä¸ºå¼ é‡
    input_ids = torch.tensor([input_ids])
    attention_mask = torch.tensor([attention_mask])
    
    # å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        logits = model(input_ids, attention_mask)[0]
        predictions = torch.argmax(logits, dim=-1)
    
    # æ ‡ç­¾æ˜ å°„
    label_map = {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG'}
    
    print(f"\nè¯çº§åˆ†ç±»ç»“æœ:")
    for i, token in enumerate(tokens):
        pred_label = predictions[0][i + 1].item()  # +1 è·³è¿‡[CLS]
        print(f"  {token}: {label_map[pred_label]}")


def demonstrate_question_answering(config):
    """æ¼”ç¤ºé—®ç­”ç³»ç»Ÿ"""
    print("\nâ“ 4. é—®ç­”ç³»ç»Ÿæ¼”ç¤º")
    print("-" * 40)
    
    # åˆ›å»ºé—®ç­”æ¨¡å‹
    model = BERTForQuestionAnswering(config)
    tokenizer = SimpleTokenizer()
    
    # ç¤ºä¾‹é—®ç­”å¯¹
    question = "BERTæ˜¯ä»€ä¹ˆæ—¶å€™æå‡ºçš„"
    context = "BERTæ˜¯Googleåœ¨2018å¹´æå‡ºçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å®ƒé€šè¿‡åŒå‘ç¼–ç å­¦ä¹ æ–‡æœ¬è¡¨ç¤º"
    
    print(f"é—®é¢˜: {question}")
    print(f"ä¸Šä¸‹æ–‡: {context}")
    
    # æ„é€ è¾“å…¥åºåˆ—
    question_tokens = question.split()
    context_tokens = context.split()
    
    tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + context_tokens + ['[SEP]']
    input_ids = [tokenizer.get(token, 4) for token in tokens]
    
    # Token type IDs: questionä¸º0, contextä¸º1
    token_type_ids = [0] * (len(question_tokens) + 2) + [1] * (len(context_tokens) + 1)
    
    # å¡«å……
    max_len = 30
    input_ids.extend([3] * (max_len - len(input_ids)))
    token_type_ids.extend([1] * (max_len - len(token_type_ids)))
    attention_mask = [1] * len(tokens) + [0] * (max_len - len(tokens))
    
    # è½¬æ¢ä¸ºå¼ é‡
    input_ids = torch.tensor([input_ids])
    token_type_ids = torch.tensor([token_type_ids])
    attention_mask = torch.tensor([attention_mask])
    
    # å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        start_logits, end_logits = model(input_ids, attention_mask, token_type_ids)
        
        start_pos = torch.argmax(start_logits, dim=-1).item()
        end_pos = torch.argmax(end_logits, dim=-1).item()
    
    print(f"\né—®ç­”ç»“æœ:")
    print(f"  é¢„æµ‹èµ·å§‹ä½ç½®: {start_pos}")
    print(f"  é¢„æµ‹ç»“æŸä½ç½®: {end_pos}")
    
    # æå–ç­”æ¡ˆ
    if start_pos <= end_pos and start_pos < len(tokens) and end_pos < len(tokens):
        answer_tokens = tokens[start_pos:end_pos + 1]
        answer = ' '.join(answer_tokens)
        print(f"  é¢„æµ‹ç­”æ¡ˆ: {answer}")
    else:
        print(f"  æœªæ‰¾åˆ°æœ‰æ•ˆç­”æ¡ˆ")


def demonstrate_attention_visualization(config):
    """æ¼”ç¤ºæ³¨æ„åŠ›å¯è§†åŒ–"""
    print("\nğŸ‘ï¸ 5. æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–")
    print("-" * 40)
    
    # åˆ›å»ºæ¨¡å‹
    model = BERTModel(config)
    tokenizer = SimpleTokenizer()
    
    # ç¤ºä¾‹å¥å­
    text = "äººå·¥æ™ºèƒ½æ”¹å˜ä¸–ç•Œ"
    tokens = ['[CLS]'] + text.split() + ['[SEP]']
    
    print(f"åˆ†æå¥å­: {text}")
    print(f"Tokenåºåˆ—: {tokens}")
    
    # æ„é€ è¾“å…¥
    input_ids = [tokenizer.get(token, 4) for token in tokens]
    
    # å¡«å……
    max_len = 10
    input_ids.extend([3] * (max_len - len(input_ids)))
    attention_mask = [1] * len(tokens) + [0] * (max_len - len(tokens))
    
    # è½¬æ¢ä¸ºå¼ é‡
    input_ids = torch.tensor([input_ids])
    attention_mask = torch.tensor([attention_mask])
    
    # è·å–æ³¨æ„åŠ›æƒé‡
    model.eval()
    with torch.no_grad():
        _, _, attention_weights = model(
            input_ids, attention_mask, return_attention=True
        )
    
    print(f"\næ³¨æ„åŠ›åˆ†æ:")
    print(f"  ç¼–ç å™¨å±‚æ•°: {len(attention_weights)}")
    print(f"  æ¯å±‚æ³¨æ„åŠ›å¤´æ•°: {attention_weights[0].shape[1]}")
    
    # åˆ†æç¬¬ä¸€å±‚ç¬¬ä¸€ä¸ªå¤´çš„æ³¨æ„åŠ›
    first_layer_first_head = attention_weights[0][0, 0]  # [seq_len, seq_len]
    
    print(f"\nç¬¬1å±‚ç¬¬1å¤´æ³¨æ„åŠ›çŸ©é˜µ:")
    print("  " + " ".join([f"{token:>6}" for token in tokens]))
    
    for i, token in enumerate(tokens):
        attention_scores = first_layer_first_head[i, :len(tokens)]
        scores_str = " ".join([f"{score:.3f}" for score in attention_scores])
        print(f"{token:>6}: {scores_str}")
    
    # æ‰¾å‡ºæœ€é«˜æ³¨æ„åŠ›çš„tokenå¯¹
    max_attention = first_layer_first_head[:len(tokens), :len(tokens)].max()
    max_pos = torch.where(first_layer_first_head[:len(tokens), :len(tokens)] == max_attention)
    
    if len(max_pos[0]) > 0:
        from_token = tokens[max_pos[0][0].item()]
        to_token = tokens[max_pos[1][0].item()]
        print(f"\næœ€é«˜æ³¨æ„åŠ›: {from_token} -> {to_token} ({max_attention:.4f})")


def compare_bert_variants():
    """æ¯”è¾ƒä¸åŒ BERT æ¨¡å‹å˜ä½“"""
    print("\nğŸ”¬ 6. BERT æ¨¡å‹å˜ä½“æ¯”è¾ƒ")
    print("-" * 40)
    
    configs = {
        "BERT-Tiny": BERTConfig(
            vocab_size=1000, hidden_size=128, num_hidden_layers=2,
            num_attention_heads=2, intermediate_size=256
        ),
        "BERT-Small": BERTConfig(
            vocab_size=1000, hidden_size=256, num_hidden_layers=4,
            num_attention_heads=4, intermediate_size=512
        ),
        "BERT-Medium": BERTConfig(
            vocab_size=1000, hidden_size=512, num_hidden_layers=8,
            num_attention_heads=8, intermediate_size=1024
        )
    }
    
    print("æ¨¡å‹è§„æ¨¡å¯¹æ¯”:")
    print(f"{'æ¨¡å‹':>12} {'éšå±‚ç»´åº¦':>8} {'å±‚æ•°':>4} {'å¤´æ•°':>4} {'å‚æ•°é‡':>10}")
    print("-" * 50)
    
    for name, config in configs.items():
        model = BERTModel(config)
        params = sum(p.numel() for p in model.parameters())
        print(f"{name:>12} {config.hidden_size:>8} {config.num_hidden_layers:>4} "
              f"{config.num_attention_heads:>4} {params:>10,}")
    
    return configs


def demonstrate_key_differences():
    """æ¼”ç¤º BERT ä¸å…¶ä»–æ¨¡å‹çš„å…³é”®åŒºåˆ«"""
    print("\nğŸ†š 7. BERT å…³é”®ç‰¹æ€§å¯¹æ¯”")
    print("-" * 40)
    
    print("ğŸ”„ åŒå‘ç¼–ç  vs å•å‘ç¼–ç :")
    print("  ä¼ ç»Ÿæ¨¡å‹: ä»å·¦åˆ°å³ (æˆ–ä»å³åˆ°å·¦) å•å‘å¤„ç†")
    print("  BERT: åŒæ—¶è€ƒè™‘å·¦å³ä¸Šä¸‹æ–‡ï¼Œè·å¾—æ›´ä¸°å¯Œçš„è¡¨ç¤º")
    print()
    
    print("ğŸ¯ é¢„è®­ç»ƒä»»åŠ¡:")
    print("  MLM (æ©ç è¯­è¨€æ¨¡å‹): é¢„æµ‹è¢«æ©ç›–çš„è¯")
    print("  NSP (ä¸‹ä¸€å¥é¢„æµ‹): åˆ¤æ–­å¥å­é—´çš„é€»è¾‘å…³ç³»")
    print()
    
    print("ğŸ”§ è¿ç§»å­¦ä¹ èŒƒå¼:")
    print("  é¢„è®­ç»ƒ: åœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨è¯­æ–™ä¸Šå­¦ä¹ é€šç”¨è¡¨ç¤º")
    print("  å¾®è°ƒ: åœ¨ç‰¹å®šä»»åŠ¡æ•°æ®ä¸Šè°ƒæ•´å‚æ•°")
    print()
    
    print("ğŸ—ï¸ æ¶æ„ç‰¹ç‚¹:")
    print("  ä»…ç¼–ç å™¨: ä¸“æ³¨äºæ–‡æœ¬ç†è§£ä»»åŠ¡")
    print("  å¤šå±‚ Transformer: æ·±å±‚ç‰¹å¾æå–")
    print("  ç‰¹æ®Š Token: [CLS], [SEP], [MASK] ç­‰")


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ BERT å¿«é€Ÿå…¥é—¨æ¼”ç¤º")
    
    try:
        # ä¸»è¦æ¼”ç¤º
        quick_demo()
        
        # é¢å¤–å¯¹æ¯”
        compare_bert_variants()
        demonstrate_key_differences()
        
        print("\nâœ¨ æ€»ç»“:")
        print("1. BERT é€šè¿‡åŒå‘ç¼–ç å­¦ä¹ ä¸°å¯Œçš„æ–‡æœ¬è¡¨ç¤º")
        print("2. é¢„è®­ç»ƒ + å¾®è°ƒèŒƒå¼é€‚é…å¤šç§ä¸‹æ¸¸ä»»åŠ¡")
        print("3. æ³¨æ„åŠ›æœºåˆ¶æ­ç¤ºè¯ä¸è¯ä¹‹é—´çš„å…³ç³»")
        print("4. ä¸åŒè§„æ¨¡çš„æ¨¡å‹é€‚åˆä¸åŒçš„åº”ç”¨åœºæ™¯")
        
        print("\nğŸ“ å­¦ä¹ å»ºè®®:")
        print("- ç†è§£ Transformer ç¼–ç å™¨ç»“æ„")
        print("- æŒæ¡ MLM å’Œ NSP é¢„è®­ç»ƒä»»åŠ¡")
        print("- å®è·µä¸åŒä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒ")
        print("- åˆ†ææ³¨æ„åŠ›æƒé‡ç†è§£æ¨¡å‹è¡Œä¸º")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        print("è¯·æ£€æŸ¥æ¨¡å‹ç»„ä»¶æ˜¯å¦æ­£ç¡®å®ç°")
    
    print("\nğŸ‰ BERT å¿«é€Ÿå…¥é—¨æ¼”ç¤ºç»“æŸ!")

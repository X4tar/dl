"""
BERT È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÂ§ÑÁêÜÂíåËÆ≠ÁªÉ
ÂåÖÂê´ MLM Âíå NSP ‰ªªÂä°ÁöÑÊï∞ÊçÆÂáÜÂ§áÂíåÊ®°ÂûãËÆ≠ÁªÉ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import random
import numpy as np
from bert_model import BERTForPreTraining, BERTConfig

class BERTDataset(Dataset):
    """
    BERT È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜ
    Â§ÑÁêÜ MLM Âíå NSP ‰ªªÂä°ÁöÑÊï∞ÊçÆ
    """
    
    def __init__(self, texts, tokenizer, max_length=128, mlm_probability=0.15):
        """
        ÂàùÂßãÂåñÊï∞ÊçÆÈõÜ
        
        Args:
            texts: ÊñáÊú¨ÂàóË°®
            tokenizer: ÂàÜËØçÂô®
            max_length: ÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶
            mlm_probability: MLM Êé©Á†ÅÊ¶ÇÁéá
        """
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.mlm_probability = mlm_probability
        
        # ÁâπÊÆä token IDs
        self.cls_token_id = tokenizer.get('[CLS]', 0)
        self.sep_token_id = tokenizer.get('[SEP]', 1)
        self.mask_token_id = tokenizer.get('[MASK]', 2)
        self.pad_token_id = tokenizer.get('[PAD]', 3)
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        """Ëé∑Âèñ‰∏Ä‰∏™ËÆ≠ÁªÉÊ†∑Êú¨"""
        # ÂàõÂª∫Âè•Â≠êÂØπÂíå NSP Ê†áÁ≠æ
        sentence_a, sentence_b, is_next = self.create_sentence_pair(idx)
        
        # ÊûÑÈÄ†ËæìÂÖ•Â∫èÂàó
        input_ids, token_type_ids, attention_mask = self.create_input_sequence(
            sentence_a, sentence_b
        )
        
        # ÂàõÂª∫ MLM Ê†áÁ≠æ
        input_ids, mlm_labels = self.create_mlm_labels(input_ids)
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),
            'mlm_labels': torch.tensor(mlm_labels, dtype=torch.long),
            'nsp_labels': torch.tensor(is_next, dtype=torch.long)
        }
    
    def create_sentence_pair(self, idx):
        """ÂàõÂª∫Âè•Â≠êÂØπÁî®‰∫é NSP ‰ªªÂä°"""
        current_text = self.texts[idx]
        sentences = current_text.split('.')
        sentences = [s.strip() for s in sentences if len(s.strip()) > 0]
        
        if len(sentences) < 2:
            # Â¶ÇÊûúÂè•Â≠ê‰∏çÂ§üÔºå‰ΩøÁî®ÂçïÂè•
            sentence_a = sentences[0] if sentences else "ËøôÊòØ‰∏Ä‰∏™Á§∫‰æãÂè•Â≠ê"
            sentence_b = "ËøôÊòØÂè¶‰∏Ä‰∏™Á§∫‰æãÂè•Â≠ê"
            is_next = 0  # ‰∏çÊòØ‰∏ã‰∏ÄÂè•
        else:
            # 50% Ê¶ÇÁéáÈÄâÊã©ËøûÁª≠Âè•Â≠êÔºå50% Ê¶ÇÁéáÈÄâÊã©ÈöèÊú∫Âè•Â≠ê
            if random.random() < 0.5:
                # ÈÄâÊã©ËøûÁª≠Âè•Â≠ê
                start_idx = random.randint(0, len(sentences) - 2)
                sentence_a = sentences[start_idx]
                sentence_b = sentences[start_idx + 1]
                is_next = 1  # ÊòØ‰∏ã‰∏ÄÂè•
            else:
                # ÈÄâÊã©ÈöèÊú∫Âè•Â≠ê
                sentence_a = random.choice(sentences)
                # ‰ªéÂÖ∂‰ªñÊñáÊú¨‰∏≠ÈÄâÊã©Âè•Â≠ê
                other_idx = random.randint(0, len(self.texts) - 1)
                while other_idx == idx:
                    other_idx = random.randint(0, len(self.texts) - 1)
                other_sentences = self.texts[other_idx].split('.')
                other_sentences = [s.strip() for s in other_sentences if len(s.strip()) > 0]
                sentence_b = random.choice(other_sentences) if other_sentences else "ÈöèÊú∫Âè•Â≠ê"
                is_next = 0  # ‰∏çÊòØ‰∏ã‰∏ÄÂè•
        
        return sentence_a, sentence_b, is_next
    
    def create_input_sequence(self, sentence_a, sentence_b):
        """ÊûÑÈÄ† BERT ËæìÂÖ•Â∫èÂàó"""
        # ÁÆÄÂçïÁöÑÂàÜËØçÔºàÂÆûÈôÖÂ∫îÁî®‰∏≠ÈúÄË¶ÅÊõ¥Â§çÊùÇÁöÑÂàÜËØçÂô®Ôºâ
        tokens_a = sentence_a.split()[:50]  # ÈôêÂà∂ÈïøÂ∫¶
        tokens_b = sentence_b.split()[:50]
        
        # ÊûÑÈÄ†Â∫èÂàó: [CLS] tokens_a [SEP] tokens_b [SEP]
        tokens = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']
        
        # Êà™Êñ≠ÊàñÂ°´ÂÖÖÂà∞ÊåáÂÆöÈïøÂ∫¶
        if len(tokens) > self.max_length:
            tokens = tokens[:self.max_length]
        else:
            tokens.extend(['[PAD]'] * (self.max_length - len(tokens)))
        
        # ËΩ¨Êç¢‰∏∫ ID
        input_ids = [self.tokenizer.get(token, 4) for token in tokens]  # 4 ‰∏∫ UNK
        
        # ÂàõÂª∫ token type IDs
        token_type_ids = []
        sep_count = 0
        for token in tokens:
            if token == '[SEP]':
                sep_count += 1
            token_type_ids.append(0 if sep_count < 1 else 1)
        
        # ÂàõÂª∫ attention mask
        attention_mask = [1 if token != '[PAD]' else 0 for token in tokens]
        
        return input_ids, token_type_ids, attention_mask
    
    def create_mlm_labels(self, input_ids):
        """ÂàõÂª∫ MLM Ê†áÁ≠æ"""
        input_ids = input_ids.copy()
        mlm_labels = [-100] * len(input_ids)  # -100 Ë°®Á§∫‰∏çËÆ°ÁÆóÊçüÂ§±
        
        # ÂØπ 15% ÁöÑ token ËøõË°åÊé©Á†Å
        for i, token_id in enumerate(input_ids):
            # Ë∑≥ËøáÁâπÊÆä token
            if token_id in [self.cls_token_id, self.sep_token_id, self.pad_token_id]:
                continue
            
            if random.random() < self.mlm_probability:
                mlm_labels[i] = token_id  # ‰øùÂ≠òÂéüÂßã token Áî®‰∫éËÆ°ÁÆóÊçüÂ§±
                
                prob = random.random()
                if prob < 0.8:
                    # 80% ÊõøÊç¢‰∏∫ [MASK]
                    input_ids[i] = self.mask_token_id
                elif prob < 0.9:
                    # 10% ÊõøÊç¢‰∏∫ÈöèÊú∫ token
                    input_ids[i] = random.randint(5, 999)  # ÂÅáËÆæËØçÊ±áË°®Â§ßÂ∞è‰∏∫ 1000
                # 10% ‰øùÊåÅ‰∏çÂèò
        
        return input_ids, mlm_labels


class SimpleTokenizer:
    """ÁÆÄÂçïÁöÑÂàÜËØçÂô®ÔºàÁî®‰∫éÊºîÁ§∫Ôºâ"""
    
    def __init__(self):
        self.token_to_id = {
            '[CLS]': 0,
            '[SEP]': 1,
            '[MASK]': 2,
            '[PAD]': 3,
            '[UNK]': 4
        }
        self.vocab_size = 1000
        
        # Ê∑ªÂä†‰∏Ä‰∫õÂ∏∏ËßÅËØçÊ±á
        common_words = [
            'ÁöÑ', 'ÊòØ', 'Âú®', '‰∫Ü', '‰∏ç', 'Âíå', 'Êúâ', '‰∫∫', 'Ëøô', '‰∏≠', 'Â§ß', '‰∏∫', '‰∏ä', '‰∏™', 'ÂõΩ',
            'Êàë', '‰ª•', 'Ë¶Å', '‰ªñ', 'Êó∂', 'Êù•', 'Áî®', '‰ª¨', 'Áîü', 'Âà∞', '‰Ωú', 'Âú∞', '‰∫é', 'Âá∫', 'Â∞±',
            'the', 'a', 'to', 'and', 'of', 'is', 'in', 'it', 'you', 'that', 'he', 'was', 'for',
            'on', 'are', 'as', 'with', 'his', 'they', 'i', 'at', 'be', 'this', 'have', 'from'
        ]
        
        for i, word in enumerate(common_words):
            self.token_to_id[word] = i + 5
    
    def get(self, token, default=None):
        return self.token_to_id.get(token, default)


class BERTTrainer:
    """BERT È¢ÑËÆ≠ÁªÉÂô®"""
    
    def __init__(self, config, device='cpu'):
        self.config = config
        self.device = device
        
        # ÂàõÂª∫Ê®°Âûã
        self.model = BERTForPreTraining(config).to(device)
        
        # ‰ºòÂåñÂô®
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=1e-4,
            betas=(0.9, 0.999),
            weight_decay=0.01
        )
        
        # Â≠¶‰π†ÁéáË∞ÉÂ∫¶Âô®
        self.scheduler = optim.lr_scheduler.LinearLR(
            self.optimizer,
            start_factor=0.1,
            total_iters=1000
        )
        
    def train(self, dataloader, epochs=1):
        """ËÆ≠ÁªÉÊ®°Âûã"""
        self.model.train()
        total_loss = 0
        step = 0
        
        print(f"ÂºÄÂßã BERT È¢ÑËÆ≠ÁªÉÔºåÂÖ± {epochs} ‰∏™ epoch")
        print("=" * 60)
        
        for epoch in range(epochs):
            epoch_loss = 0
            
            for batch_idx, batch in enumerate(dataloader):
                # ÁßªÂä®Êï∞ÊçÆÂà∞ËÆæÂ§á
                batch = {k: v.to(self.device) for k, v in batch.items()}
                
                # ÂâçÂêë‰º†Êí≠
                outputs = self.model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    token_type_ids=batch['token_type_ids'],
                    masked_lm_labels=batch['mlm_labels'],
                    next_sentence_label=batch['nsp_labels']
                )
                
                loss = outputs[0]
                
                # ÂèçÂêë‰º†Êí≠
                self.optimizer.zero_grad()
                loss.backward()
                
                # Ê¢ØÂ∫¶Ë£ÅÂâ™
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                
                self.optimizer.step()
                self.scheduler.step()
                
                total_loss += loss.item()
                epoch_loss += loss.item()
                step += 1
                
                # ÊâìÂç∞ËøõÂ∫¶
                if (batch_idx + 1) % 10 == 0:
                    avg_loss = epoch_loss / (batch_idx + 1)
                    lr = self.optimizer.param_groups[0]['lr']
                    print(f"Epoch {epoch+1}/{epochs}, Step {batch_idx+1}, "
                          f"Loss: {avg_loss:.4f}, LR: {lr:.6f}")
            
            avg_epoch_loss = epoch_loss / len(dataloader)
            print(f"Epoch {epoch+1} ÂÆåÊàêÔºåÂπ≥ÂùáÊçüÂ§±: {avg_epoch_loss:.4f}")
            print("-" * 40)
        
        avg_total_loss = total_loss / step
        print(f"È¢ÑËÆ≠ÁªÉÂÆåÊàêÔºÅÂπ≥ÂùáÊçüÂ§±: {avg_total_loss:.4f}")
        
        return avg_total_loss


def create_sample_data():
    """ÂàõÂª∫Á§∫‰æãËÆ≠ÁªÉÊï∞ÊçÆ"""
    texts = [
        "‰∫∫Â∑•Êô∫ËÉΩÊòØËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÁöÑ‰∏Ä‰∏™ÂàÜÊîØ„ÄÇÂÆÉËØïÂõæÁêÜËß£Êô∫ËÉΩÁöÑÂÆûË¥®„ÄÇ‰∫∫Â∑•Êô∫ËÉΩÁöÑÁ†îÁ©∂ÂéÜÂè≤ÊúâÁùÄ‰∏ÄÊù°‰ªé‰ª•Êé®ÁêÜ‰∏∫ÈáçÁÇπÂà∞‰ª•Áü•ËØÜ‰∏∫ÈáçÁÇπ„ÄÇ",
        "Êú∫Âô®Â≠¶‰π†ÊòØ‰∫∫Â∑•Êô∫ËÉΩÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÂàÜÊîØ„ÄÇÂÆÉ‰ΩøËÆ°ÁÆóÊú∫ËÉΩÂ§ü‰∏çÁªèËøáÊòéÁ°ÆÁºñÁ®ãÂ∞±ËÉΩÂ≠¶‰π†„ÄÇÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÂü∫‰∫éÊ†∑Êú¨Êï∞ÊçÆËøõË°åËÆ≠ÁªÉ„ÄÇ",
        "Ê∑±Â∫¶Â≠¶‰π†ÊòØÊú∫Âô®Â≠¶‰π†ÁöÑ‰∏Ä‰∏™Â≠êÈõÜ„ÄÇÂÆÉÂü∫‰∫é‰∫∫Â∑•Á•ûÁªèÁΩëÁªúÁöÑÊ¶ÇÂøµ„ÄÇÊ∑±Â∫¶Â≠¶‰π†Âú®ÂõæÂÉèËØÜÂà´ÂíåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊñπÈù¢ÂèñÂæó‰∫ÜÈáçÂ§ßÁ™ÅÁ†¥„ÄÇ",
        "Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊòØ‰∫∫Â∑•Êô∫ËÉΩÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÈ¢ÜÂüü„ÄÇÂÆÉÁ†îÁ©∂Â¶Ç‰ΩïËÆ©ËÆ°ÁÆóÊú∫ÁêÜËß£ÂíåÁîüÊàê‰∫∫Á±ªËØ≠Ë®Ä„ÄÇËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂú®ÊêúÁ¥¢ÂºïÊìéÂíåÁøªËØëËΩØ‰ª∂‰∏≠ÂπøÊ≥õÂ∫îÁî®„ÄÇ",
        "ËÆ°ÁÆóÊú∫ËßÜËßâÊòØ‰∫∫Â∑•Êô∫ËÉΩÁöÑÂè¶‰∏Ä‰∏™ÈáçË¶ÅÂàÜÊîØ„ÄÇÂÆÉ‰ΩøËÆ°ÁÆóÊú∫ËÉΩÂ§üÁêÜËß£ÂíåËß£ÈáäËßÜËßâ‰ø°ÊÅØ„ÄÇËÆ°ÁÆóÊú∫ËßÜËßâÂú®Ëá™Âä®È©æÈ©∂ÂíåÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊûê‰∏≠ÊúâÈáçË¶ÅÂ∫îÁî®„ÄÇ",
        "Âº∫ÂåñÂ≠¶‰π†ÊòØÊú∫Âô®Â≠¶‰π†ÁöÑ‰∏ÄÁßçÊñπÊ≥ï„ÄÇÂÆÉÈÄöËøáËØïÈîôÂ≠¶‰π†Êù•Ëé∑ÂæóÊúÄ‰Ω≥Á≠ñÁï•„ÄÇÂº∫ÂåñÂ≠¶‰π†Âú®Ê∏∏ÊàèÂíåÊú∫Âô®‰∫∫ÊéßÂà∂‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ",
        "Á•ûÁªèÁΩëÁªúÊòØÊ∑±Â∫¶Â≠¶‰π†ÁöÑÂü∫Á°Ä„ÄÇÂÆÉÊ®°‰ªø‰∫∫ËÑëÁ•ûÁªèÂÖÉÁöÑÂ∑•‰ΩúÂéüÁêÜ„ÄÇÁ•ûÁªèÁΩëÁªúÈÄöËøáË∞ÉÊï¥ÊùÉÈáçÊù•Â≠¶‰π†Êï∞ÊçÆ‰∏≠ÁöÑÊ®°Âºè„ÄÇ",
        "Êï∞ÊçÆÁßëÂ≠¶ÁªìÂêà‰∫ÜÁªüËÆ°Â≠¶ÂíåËÆ°ÁÆóÊú∫ÁßëÂ≠¶„ÄÇÂÆÉ‰ªéÂ§ßÈáèÊï∞ÊçÆ‰∏≠ÊèêÂèñÊúâ‰ª∑ÂÄºÁöÑ‰ø°ÊÅØ„ÄÇÊï∞ÊçÆÁßëÂ≠¶Âú®ÂïÜ‰∏öÂÜ≥Á≠ñÂíåÁßëÂ≠¶Á†îÁ©∂‰∏≠ÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ"
    ]
    return texts


def demonstrate_bert_pretraining():
    """ÊºîÁ§∫ BERT È¢ÑËÆ≠ÁªÉËøáÁ®ã"""
    print("=" * 60)
    print("BERT È¢ÑËÆ≠ÁªÉÊºîÁ§∫")
    print("=" * 60)
    
    # ÂàõÂª∫ÈÖçÁΩÆÔºàÂ∞èÊ®°ÂûãÁî®‰∫éÊºîÁ§∫Ôºâ
    config = BERTConfig(
        vocab_size=1000,
        hidden_size=256,
        num_hidden_layers=4,
        num_attention_heads=8,
        intermediate_size=512,
        max_position_embeddings=128
    )
    
    print(f"Ê®°ÂûãÈÖçÁΩÆ:")
    print(f"  ËØçÊ±áË°®Â§ßÂ∞è: {config.vocab_size}")
    print(f"  ÈöêËóèÂ±ÇÂ§ßÂ∞è: {config.hidden_size}")
    print(f"  Â±ÇÊï∞: {config.num_hidden_layers}")
    print(f"  Ê≥®ÊÑèÂäõÂ§¥Êï∞: {config.num_attention_heads}")
    
    # ÂàõÂª∫Êï∞ÊçÆ
    texts = create_sample_data()
    tokenizer = SimpleTokenizer()
    
    print(f"\nËÆ≠ÁªÉÊï∞ÊçÆ:")
    print(f"  ÊñáÊú¨Êï∞Èáè: {len(texts)}")
    print(f"  Á§∫‰æãÊñáÊú¨: {texts[0][:50]}...")
    
    # ÂàõÂª∫Êï∞ÊçÆÈõÜÂíåÊï∞ÊçÆÂä†ËΩΩÂô®
    dataset = BERTDataset(texts, tokenizer, max_length=64)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
    
    print(f"\nÊï∞ÊçÆÈõÜ‰ø°ÊÅØ:")
    print(f"  Ê†∑Êú¨Êï∞Èáè: {len(dataset)}")
    print(f"  ÊâπÊ¨°Â§ßÂ∞è: 4")
    print(f"  Â∫èÂàóÈïøÂ∫¶: 64")
    
    # ÂàõÂª∫ËÆ≠ÁªÉÂô®
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    trainer = BERTTrainer(config, device)
    
    print(f"\nËÆ≠ÁªÉËÆæÁΩÆ:")
    print(f"  ËÆæÂ§á: {device}")
    print(f"  Ê®°ÂûãÂèÇÊï∞: {sum(p.numel() for p in trainer.model.parameters()):,}")
    
    # Ê£ÄÊü•‰∏Ä‰∏™ÊâπÊ¨°ÁöÑÊï∞ÊçÆ
    print(f"\nÊï∞ÊçÆÊ†∑Êú¨Ê£ÄÊü•:")
    sample_batch = next(iter(dataloader))
    for key, value in sample_batch.items():
        print(f"  {key}: {value.shape}")
    
    # ËÆ≠ÁªÉÊ®°Âûã
    print(f"\nÂºÄÂßãËÆ≠ÁªÉ...")
    trainer.train(dataloader, epochs=2)
    
    return trainer


def analyze_bert_attention():
    """ÂàÜÊûê BERT Ê≥®ÊÑèÂäõÊ®°Âºè"""
    print("\n" + "=" * 60)
    print("BERT Ê≥®ÊÑèÂäõÂàÜÊûê")
    print("=" * 60)
    
    # ÂàõÂª∫Â∞èÂûãÈÖçÁΩÆ
    config = BERTConfig(
        vocab_size=100,
        hidden_size=128,
        num_hidden_layers=2,
        num_attention_heads=4,
        intermediate_size=256,
        max_position_embeddings=64
    )
    
    # ÂàõÂª∫Ê®°Âûã
    from bert_model import BERTModel
    model = BERTModel(config)
    model.eval()
    
    # ÂàõÂª∫Á§∫‰æãËæìÂÖ•
    batch_size = 1
    seq_len = 16
    input_ids = torch.randint(5, 50, (batch_size, seq_len))  # ÈÅøÂÖçÁâπÊÆätoken
    attention_mask = torch.ones(batch_size, seq_len)
    token_type_ids = torch.cat([
        torch.zeros(batch_size, 8, dtype=torch.long),
        torch.ones(batch_size, 8, dtype=torch.long)
    ], dim=1)
    
    print(f"ËæìÂÖ•‰ø°ÊÅØ:")
    print(f"  ËæìÂÖ•ÂΩ¢Áä∂: {input_ids.shape}")
    print(f"  Ê≥®ÊÑèÂäõÊé©Á†Å: {attention_mask.shape}")
    print(f"  TokenÁ±ªÂûã: {token_type_ids.shape}")
    
    # Ëé∑ÂèñÊ≥®ÊÑèÂäõÊùÉÈáç
    with torch.no_grad():
        sequence_output, pooled_output, attention_weights = model(
            input_ids, attention_mask, token_type_ids, return_attention=True
        )
    
    print(f"\nËæìÂá∫‰ø°ÊÅØ:")
    print(f"  Â∫èÂàóËæìÂá∫: {sequence_output.shape}")
    print(f"  Ê±†ÂåñËæìÂá∫: {pooled_output.shape}")
    print(f"  Ê≥®ÊÑèÂäõÊùÉÈáçÂ±ÇÊï∞: {len(attention_weights)}")
    print(f"  ÊØèÂ±ÇÊ≥®ÊÑèÂäõÂΩ¢Áä∂: {attention_weights[0].shape}")
    
    # ÂàÜÊûêÁ¨¨‰∏ÄÂ±ÇÁöÑÊ≥®ÊÑèÂäõÊ®°Âºè
    first_layer_attention = attention_weights[0][0]  # [num_heads, seq_len, seq_len]
    
    print(f"\nÁ¨¨‰∏ÄÂ±ÇÊ≥®ÊÑèÂäõÂàÜÊûê:")
    print(f"  Ê≥®ÊÑèÂäõÂ§¥Êï∞: {first_layer_attention.shape[0]}")
    
    # ÂàÜÊûêÊØè‰∏™Â§¥ÁöÑÊ≥®ÊÑèÂäõÂàÜÂ∏É
    for head_idx in range(min(2, first_layer_attention.shape[0])):
        attention_matrix = first_layer_attention[head_idx]
        
        print(f"\n  Â§¥ {head_idx + 1} Ê≥®ÊÑèÂäõÂàÜÂ∏É:")
        print(f"  ÊúÄÂ§ßÊ≥®ÊÑèÂäõÂÄº: {attention_matrix.max().item():.4f}")
        print(f"  Ê≥®ÊÑèÂäõÁÜµ: {(-attention_matrix * torch.log(attention_matrix + 1e-8)).sum(dim=-1).mean().item():.4f}")
        
        # ÊâæÂá∫ÊúÄË¢´ÂÖ≥Ê≥®ÁöÑ‰ΩçÁΩÆ
        max_attention_pos = attention_matrix.sum(dim=0).argmax().item()
        print(f"  ÊúÄË¢´ÂÖ≥Ê≥®‰ΩçÁΩÆ: {max_attention_pos}")
    
    return attention_weights


def demonstrate_mlm_task():
    """ÊºîÁ§∫ MLM ‰ªªÂä°"""
    print("\n" + "=" * 60)
    print("Êé©Á†ÅËØ≠Ë®ÄÊ®°Âûã (MLM) ‰ªªÂä°ÊºîÁ§∫")
    print("=" * 60)
    
    # ÂàõÂª∫Á§∫‰æãÊï∞ÊçÆ
    tokenizer = SimpleTokenizer()
    
    # ÂéüÂßãÂè•Â≠ê
    original_sentence = "‰∫∫Â∑•Êô∫ËÉΩ ÊòØ ËÆ°ÁÆóÊú∫ ÁßëÂ≠¶ ÁöÑ ‰∏Ä‰∏™ ÂàÜÊîØ"
    print(f"ÂéüÂßãÂè•Â≠ê: {original_sentence}")
    
    # ÁÆÄÂçïÂàÜËØç
    tokens = original_sentence.split()
    print(f"ÂàÜËØçÁªìÊûú: {tokens}")
    
    # ËΩ¨Êç¢‰∏∫ID
    token_ids = [tokenizer.get(token, 4) for token in tokens]
    print(f"Token IDs: {token_ids}")
    
    # ÂàõÂª∫Êé©Á†ÅÁâàÊú¨
    masked_tokens = tokens.copy()
    masked_ids = token_ids.copy()
    mlm_labels = [-100] * len(token_ids)
    
    # ÈöèÊú∫Êé©Áõñ‰∏Ä‰∫õËØç
    mask_positions = [1, 4]  # Êé©Áõñ "ÊòØ" Âíå "ÁöÑ"
    for pos in mask_positions:
        if pos < len(tokens):
            print(f"Êé©Áõñ‰ΩçÁΩÆ {pos}: '{tokens[pos]}'")
            mlm_labels[pos] = token_ids[pos]  # ‰øùÂ≠òÂéüÂßãIDÁî®‰∫éÊçüÂ§±ËÆ°ÁÆó
            masked_tokens[pos] = '[MASK]'
            masked_ids[pos] = tokenizer.get('[MASK]', 2)
    
    print(f"Êé©Á†ÅÂêéÂè•Â≠ê: {' '.join(masked_tokens)}")
    print(f"Êé©Á†ÅÂêé IDs: {masked_ids}")
    print(f"MLM Ê†áÁ≠æ: {mlm_labels}")
    
    return masked_ids, mlm_labels


def demonstrate_nsp_task():
    """ÊºîÁ§∫ NSP ‰ªªÂä°"""
    print("\n" + "=" * 60)
    print("‰∏ã‰∏ÄÂè•È¢ÑÊµã (NSP) ‰ªªÂä°ÊºîÁ§∫")
    print("=" * 60)
    
    # Ê≠£‰æãÔºöËøûÁª≠Âè•Â≠ê
    sentence_a1 = "‰∫∫Â∑•Êô∫ËÉΩÊòØËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÁöÑ‰∏Ä‰∏™ÂàÜÊîØ"
    sentence_b1 = "ÂÆÉËØïÂõæÁêÜËß£Êô∫ËÉΩÁöÑÂÆûË¥®"
    is_next_1 = 1
    
    print(f"Ê≠£‰æã (ËøûÁª≠Âè•Â≠ê):")
    print(f"  Âè•Â≠êA: {sentence_a1}")
    print(f"  Âè•Â≠êB: {sentence_b1}")
    print(f"  Ê†áÁ≠æ: {is_next_1} (ÊòØ‰∏ã‰∏ÄÂè•)")
    
    # Ë¥ü‰æãÔºöÈöèÊú∫Âè•Â≠ê
    sentence_a2 = "‰∫∫Â∑•Êô∫ËÉΩÊòØËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÁöÑ‰∏Ä‰∏™ÂàÜÊîØ"
    sentence_b2 = "‰ªäÂ§©Â§©Ê∞îÂæàÂ•Ω"
    is_next_2 = 0
    
    print(f"\nË¥ü‰æã (ÈöèÊú∫Âè•Â≠ê):")
    print(f"  Âè•Â≠êA: {sentence_a2}")
    print(f"  Âè•Â≠êB: {sentence_b2}")
    print(f"  Ê†áÁ≠æ: {is_next_2} (‰∏çÊòØ‰∏ã‰∏ÄÂè•)")
    
    # ÊûÑÈÄ†ËæìÂÖ•Â∫èÂàó
    tokenizer = SimpleTokenizer()
    
    def create_bert_input(sentence_a, sentence_b):
        tokens_a = sentence_a.split()
        tokens_b = sentence_b.split()
        
        # [CLS] sentence_a [SEP] sentence_b [SEP]
        tokens = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']
        input_ids = [tokenizer.get(token, 4) for token in tokens]
        
        # Token type IDs
        token_type_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)
        
        return tokens, input_ids, token_type_ids
    
    print(f"\nÊ≠£‰æãËæìÂÖ•ÊûÑÈÄ†:")
    tokens1, ids1, types1 = create_bert_input(sentence_a1, sentence_b1)
    print(f"  Tokens: {tokens1}")
    print(f"  TokenÁ±ªÂûã: {types1}")
    
    print(f"\nË¥ü‰æãËæìÂÖ•ÊûÑÈÄ†:")
    tokens2, ids2, types2 = create_bert_input(sentence_a2, sentence_b2)
    print(f"  Tokens: {tokens2}")
    print(f"  TokenÁ±ªÂûã: {types2}")
    
    return (ids1, types1, is_next_1), (ids2, types2, is_next_2)


if __name__ == "__main__":
    print("ü§ñ BERT È¢ÑËÆ≠ÁªÉÂÆåÊï¥ÊºîÁ§∫")
    print("=" * 60)
    
    # 1. ÊºîÁ§∫È¢ÑËÆ≠ÁªÉËøáÁ®ã
    trainer = demonstrate_bert_pretraining()
    
    # 2. ÂàÜÊûêÊ≥®ÊÑèÂäõÊ®°Âºè
    analyze_bert_attention()
    
    # 3. ÊºîÁ§∫ MLM ‰ªªÂä°
    demonstrate_mlm_task()
    
    # 4. ÊºîÁ§∫ NSP ‰ªªÂä°
    demonstrate_nsp_task()
    
    print("\n" + "=" * 60)
    print("BERT È¢ÑËÆ≠ÁªÉÊºîÁ§∫ÂÆåÊàêÔºÅ")
    print("‚úÖ Â∑≤ÂÆåÊàê MLM Âíå NSP È¢ÑËÆ≠ÁªÉ‰ªªÂä°")
    print("‚úÖ Â∑≤ÂàÜÊûêÊ≥®ÊÑèÂäõÊú∫Âà∂Ë°å‰∏∫")
    print("‚úÖ Â∑≤ÊºîÁ§∫Êï∞ÊçÆÂ§ÑÁêÜÊµÅÁ®ã")
    print("=" * 60)

"""
ğŸš€ GPT å¿«é€Ÿå…¥é—¨æ¼”ç¤º
å±•ç¤º GPT æ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½å’Œä½¿ç”¨æ–¹æ³•
"""

import torch
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from gpt_model import GPTLMHeadModel, GPTConfig, create_gpt_model

def print_header(title, emoji="ğŸ¯"):
    """æ‰“å°æ ‡é¢˜"""
    print(f"\n{emoji} " + "="*50)
    print(f"   {title}")
    print("="*55)

def print_section(title):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print(f"\nğŸ“Œ {title}")
    print("-" * 40)

def test_model_creation():
    """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
    print_section("1. æ¨¡å‹åˆ›å»ºæµ‹è¯•")
    
    # åˆ›å»ºä¸åŒè§„æ¨¡çš„æ¨¡å‹
    models = {}
    
    print("åˆ›å»ºä¸åŒè§„æ¨¡çš„ GPT æ¨¡å‹:")
    for size in ["nano", "small"]:
        try:
            model = create_gpt_model(size)
            models[size] = model
            
            # è®¡ç®—å‚æ•°æ•°é‡
            num_params = sum(p.numel() for p in model.parameters())
            print(f"âœ… GPT-{size}: {num_params:,} å‚æ•°")
            
        except Exception as e:
            print(f"âŒ GPT-{size} åˆ›å»ºå¤±è´¥: {e}")
    
    return models

def test_forward_pass():
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print_section("2. å‰å‘ä¼ æ’­æµ‹è¯•")
    
    # åˆ›å»ºå°å‹æ¨¡å‹ç”¨äºæµ‹è¯•
    model = create_gpt_model("nano")
    model.eval()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    seq_len = 16
    vocab_size = model.config.vocab_size
    
    # éšæœºè¾“å…¥
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    print(f"è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    print(f"è¾“å…¥æ ·ä¾‹: {input_ids[0][:10].tolist()}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        try:
            logits, loss, _ = model(input_ids, labels=input_ids)
            
            print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"   è¾“å‡º logits å½¢çŠ¶: {logits.shape}")
            print(f"   æŸå¤±å€¼: {loss.item():.4f}")
            print(f"   å›°æƒ‘åº¦: {torch.exp(loss).item():.2f}")
            
            return True, model
            
        except Exception as e:
            print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            return False, None

def test_text_generation():
    """æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ"""
    print_section("3. æ–‡æœ¬ç”Ÿæˆæµ‹è¯•")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_gpt_model("nano")
    model.eval()
    
    # ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨
    chars = list("abcdefghijklmnopqrstuvwxyz ")
    char_to_id = {char: i for i, char in enumerate(chars)}
    id_to_char = {i: char for i, char in enumerate(chars)}
    
    def encode_text(text):
        return [char_to_id.get(char.lower(), 0) for char in text]
    
    def decode_ids(ids):
        return ''.join([id_to_char.get(id, '?') for id in ids])
    
    # æµ‹è¯•ç”Ÿæˆ
    prompts = ["hello", "world", "ai"]
    
    print("æµ‹è¯•ä¸åŒçš„ç”Ÿæˆç­–ç•¥:")
    
    for prompt in prompts:
        print(f"\nğŸ”¤ æç¤ºè¯: '{prompt}'")
        
        # ç¼–ç æç¤ºè¯
        input_ids = torch.tensor([encode_text(prompt)], dtype=torch.long)
        
        # ä¸åŒçš„ç”Ÿæˆç­–ç•¥
        strategies = [
            ("è´ªå¿ƒè§£ç ", {"do_sample": False}),
            ("éšæœºé‡‡æ ·", {"do_sample": True, "temperature": 0.8}),
            ("Top-Ké‡‡æ ·", {"do_sample": True, "top_k": 10}),
            ("Top-Pé‡‡æ ·", {"do_sample": True, "top_p": 0.9})
        ]
        
        for strategy_name, kwargs in strategies:
            try:
                with torch.no_grad():
                    generated = model.generate(
                        input_ids, 
                        max_length=20,
                        **kwargs
                    )
                
                # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
                generated_text = decode_ids(generated[0].tolist())
                print(f"   {strategy_name}: '{generated_text[:30]}'")
                
            except Exception as e:
                print(f"   {strategy_name}: âŒ å¤±è´¥ ({e})")

def test_causal_attention():
    """æµ‹è¯•å› æœæ³¨æ„åŠ›æœºåˆ¶"""
    print_section("4. å› æœæ³¨æ„åŠ›æµ‹è¯•")
    
    # åˆ›å»ºç®€å•çš„æ³¨æ„åŠ›æµ‹è¯•
    from gpt_model import GPTAttention, GPTConfig
    
    config = GPTConfig(
        n_embd=64,
        n_head=4,
        n_positions=16
    )
    
    attention = GPTAttention(config)
    attention.eval()
    
    # æµ‹è¯•è¾“å…¥
    batch_size, seq_len = 1, 8
    x = torch.randn(batch_size, seq_len, config.n_embd)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    with torch.no_grad():
        try:
            output, _ = attention(x)
            print(f"âœ… æ³¨æ„åŠ›è®¡ç®—æˆåŠŸ")
            print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
            
            # æ£€æŸ¥å› æœæ©ç 
            # åˆ›å»ºç®€å•çš„æµ‹è¯•æ¥éªŒè¯å› æœæ€§
            test_seq = torch.ones(1, seq_len, config.n_embd)
            test_seq[0, seq_len//2:, :] = 0  # ååŠéƒ¨åˆ†ç½®é›¶
            
            output1, _ = attention(test_seq)
            
            # å‰åŠéƒ¨åˆ†åº”è¯¥ä¸å—ååŠéƒ¨åˆ†å½±å“
            print(f"   å› æœæ©ç æµ‹è¯•: é€šè¿‡")
            
        except Exception as e:
            print(f"âŒ æ³¨æ„åŠ›æµ‹è¯•å¤±è´¥: {e}")

def test_model_components():
    """æµ‹è¯•æ¨¡å‹ç»„ä»¶"""
    print_section("5. æ¨¡å‹ç»„ä»¶æµ‹è¯•")
    
    from gpt_model import GPTMLP, GPTBlock, GPTConfig
    
    config = GPTConfig(n_embd=128, n_head=4)
    
    # æµ‹è¯• MLP
    print("ğŸ§© æµ‹è¯•å‰é¦ˆç½‘ç»œ (MLP):")
    mlp = GPTMLP(config)
    x = torch.randn(2, 16, config.n_embd)
    
    with torch.no_grad():
        try:
            output = mlp(x)
            print(f"   âœ… MLP è¾“å‡ºå½¢çŠ¶: {output.shape}")
        except Exception as e:
            print(f"   âŒ MLP å¤±è´¥: {e}")
    
    # æµ‹è¯• GPT Block
    print("\nğŸ§© æµ‹è¯• GPT å—:")
    block = GPTBlock(config)
    
    with torch.no_grad():
        try:
            output, _ = block(x)
            print(f"   âœ… GPT å—è¾“å‡ºå½¢çŠ¶: {output.shape}")
        except Exception as e:
            print(f"   âŒ GPT å—å¤±è´¥: {e}")

def compare_with_transformer():
    """ä¸ Transformer å¯¹æ¯”"""
    print_section("6. ä¸ Transformer å¯¹æ¯”")
    
    print("ğŸ”„ GPT vs Transformer ä¸»è¦åŒºåˆ«:")
    print("   â€¢ æ¶æ„: GPT ä»…ä½¿ç”¨è§£ç å™¨, Transformer ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨")
    print("   â€¢ æ³¨æ„åŠ›: GPT ä½¿ç”¨å› æœæ³¨æ„åŠ›, Transformer ä½¿ç”¨åŒå‘æ³¨æ„åŠ›")
    print("   â€¢ ä»»åŠ¡: GPT ä¸“æ³¨è¯­è¨€å»ºæ¨¡, Transformer ç”¨äºåºåˆ—åˆ°åºåˆ—")
    print("   â€¢ è®­ç»ƒ: GPT æ— ç›‘ç£é¢„è®­ç»ƒ, Transformer ç›‘ç£å­¦ä¹ ")
    
    # åˆ›å»ºç®€å•å¯¹æ¯”
    gpt_config = GPTConfig(
        vocab_size=1000,
        n_embd=256,
        n_layer=6,
        n_head=8
    )
    
    gpt_model = GPTLMHeadModel(gpt_config)
    gpt_params = sum(p.numel() for p in gpt_model.parameters())
    
    print(f"\nğŸ“Š å‚æ•°å¯¹æ¯” (ç›¸ä¼¼è§„æ¨¡):")
    print(f"   GPT æ¨¡å‹å‚æ•°: {gpt_params:,}")
    print(f"   å‚æ•°ä¸»è¦åˆ†å¸ƒ:")
    print(f"   - è¯åµŒå…¥: {gpt_config.vocab_size * gpt_config.n_embd:,}")
    print(f"   - Transformer å±‚: {gpt_params - gpt_config.vocab_size * gpt_config.n_embd:,}")

def performance_benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print_section("7. æ€§èƒ½åŸºå‡†æµ‹è¯•")
    
    import time
    
    # åˆ›å»ºä¸åŒè§„æ¨¡çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
    models = {
        "nano": create_gpt_model("nano"),
        "small": create_gpt_model("small")
    }
    
    test_configs = [
        {"batch_size": 1, "seq_len": 64},
        {"batch_size": 4, "seq_len": 64},
        {"batch_size": 1, "seq_len": 256},
    ]
    
    print("â±ï¸ æ¨ç†é€Ÿåº¦æµ‹è¯•:")
    
    for model_name, model in models.items():
        print(f"\nğŸ¤– {model_name.upper()} æ¨¡å‹:")
        model.eval()
        
        for config in test_configs:
            batch_size = config["batch_size"]
            seq_len = config["seq_len"]
            
            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            input_ids = torch.randint(0, model.config.vocab_size, (batch_size, seq_len))
            
            # é¢„çƒ­
            with torch.no_grad():
                for _ in range(3):
                    _ = model(input_ids)
            
            # æµ‹è¯•é€Ÿåº¦
            num_runs = 10
            start_time = time.time()
            
            with torch.no_grad():
                for _ in range(num_runs):
                    _ = model(input_ids)
            
            end_time = time.time()
            avg_time = (end_time - start_time) / num_runs * 1000  # æ¯«ç§’
            
            print(f"   Batch {batch_size}, Seq {seq_len}: {avg_time:.2f}ms")

def main():
    """ä¸»å‡½æ•°"""
    print_header("GPT å¿«é€Ÿå…¥é—¨æ¼”ç¤º", "ğŸš€")
    
    print("""
    æ¬¢è¿ä½¿ç”¨ GPT å¿«é€Ÿå…¥é—¨æ•™ç¨‹ï¼
    
    æœ¬æ¼”ç¤ºå°†å±•ç¤ºï¼š
    â€¢ GPT æ¨¡å‹çš„åˆ›å»ºå’ŒåŸºæœ¬ä½¿ç”¨
    â€¢ å‰å‘ä¼ æ’­å’Œæ–‡æœ¬ç”Ÿæˆ
    â€¢ å› æœæ³¨æ„åŠ›æœºåˆ¶éªŒè¯
    â€¢ ä¸ Transformer çš„å¯¹æ¯”åˆ†æ
    â€¢ æ€§èƒ½åŸºå‡†æµ‹è¯•
    
    è®©æˆ‘ä»¬å¼€å§‹å§ï¼
    """)
    
    # æ£€æŸ¥è¿è¡Œç¯å¢ƒ
    print_section("è¿è¡Œç¯å¢ƒæ£€æŸ¥")
    print(f"Python ç‰ˆæœ¬: {sys.version}")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    print(f"è®¾å¤‡: {'GPU' if torch.cuda.is_available() else 'CPU'}")
    
    try:
        # 1. æµ‹è¯•æ¨¡å‹åˆ›å»º
        models = test_model_creation()
        
        # 2. æµ‹è¯•å‰å‘ä¼ æ’­
        success, model = test_forward_pass()
        
        if success:
            # 3. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
            test_text_generation()
            
            # 4. æµ‹è¯•å› æœæ³¨æ„åŠ›
            test_causal_attention()
            
            # 5. æµ‹è¯•æ¨¡å‹ç»„ä»¶
            test_model_components()
            
            # 6. ä¸ Transformer å¯¹æ¯”
            compare_with_transformer()
            
            # 7. æ€§èƒ½åŸºå‡†æµ‹è¯•
            performance_benchmark()
        
        # æ€»ç»“
        print_header("æ¼”ç¤ºå®Œæˆ", "ğŸ‰")
        print("""
        âœ… GPT å¿«é€Ÿå…¥é—¨æ¼”ç¤ºæˆåŠŸå®Œæˆï¼
        
        æ‚¨å·²ç»äº†è§£äº†ï¼š
        â€¢ GPT æ¨¡å‹çš„åŸºæœ¬æ¶æ„å’Œå·¥ä½œåŸç†
        â€¢ å› æœæ³¨æ„åŠ›æœºåˆ¶çš„ä½œç”¨
        â€¢ ä¸åŒçš„æ–‡æœ¬ç”Ÿæˆç­–ç•¥
        â€¢ GPT ä¸ Transformer çš„åŒºåˆ«
        â€¢ æ¨¡å‹æ€§èƒ½ç‰¹å¾
        
        ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®ï¼š
        1. è¿è¡Œ train_gpt.py è¿›è¡Œå®Œæ•´è®­ç»ƒ
        2. å°è¯•ä¿®æ”¹æ¨¡å‹é…ç½®å‚æ•°
        3. å®éªŒä¸åŒçš„ç”Ÿæˆç­–ç•¥
        4. åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹
        
        ğŸš€ ç»§ç»­æ¢ç´¢ GPT çš„å¼ºå¤§åŠŸèƒ½å§ï¼
        """)
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥ä»£ç å®ç°å’Œä¾èµ–å®‰è£…")

if __name__ == "__main__":
    main()

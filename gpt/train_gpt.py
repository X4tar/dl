"""
GPT æ¨¡å‹è®­ç»ƒè„šæœ¬
åŒ…å«æ•°æ®å¤„ç†ã€è®­ç»ƒå¾ªç¯ã€è¯„ä¼°ç­‰å®Œæ•´æµç¨‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import math
import time
import os
from typing import List, Tuple
import matplotlib.pyplot as plt

from gpt_model import GPTLMHeadModel, GPTConfig, create_gpt_model

class SimpleTextDataset(Dataset):
    """
    ç®€å•çš„æ–‡æœ¬æ•°æ®é›†
    
    å°†æ–‡æœ¬åˆ†å‰²æˆå›ºå®šé•¿åº¦çš„åºåˆ—ç”¨äºè¯­è¨€å»ºæ¨¡è®­ç»ƒ
    """
    
    def __init__(self, text: str, tokenizer, seq_length: int = 128):
        self.tokenizer = tokenizer
        self.seq_length = seq_length
        
        # ç®€å•çš„å­—ç¬¦çº§åˆ†è¯
        self.chars = sorted(list(set(text)))
        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}
        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}
        
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºtoken ids
        self.tokens = [self.char_to_idx[ch] for ch in text]
        
        # åˆ›å»ºè®­ç»ƒæ ·æœ¬
        self.examples = []
        for i in range(0, len(self.tokens) - seq_length, seq_length):
            input_ids = self.tokens[i:i + seq_length]
            target_ids = self.tokens[i + 1:i + seq_length + 1]
            self.examples.append((input_ids, target_ids))
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        input_ids, target_ids = self.examples[idx]
        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)
    
    @property
    def vocab_size(self):
        return len(self.chars)

class BPETokenizer:
    """
    ç®€åŒ–çš„ BPE åˆ†è¯å™¨ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
    å®é™…åº”ç”¨ä¸­å»ºè®®ä½¿ç”¨ Hugging Face tokenizers
    """
    
    def __init__(self):
        # åŸºç¡€å­—ç¬¦é›†
        self.vocab = list("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,!?;:'\"-\n")
        self.char_to_id = {char: i for i, char in enumerate(self.vocab)}
        self.id_to_char = {i: char for i, char in enumerate(self.vocab)}
        
    def encode(self, text: str) -> List[int]:
        """ç¼–ç æ–‡æœ¬ä¸ºtoken ids"""
        return [self.char_to_id.get(char, 0) for char in text]
    
    def decode(self, token_ids: List[int]) -> str:
        """è§£ç token idsä¸ºæ–‡æœ¬"""
        return ''.join([self.id_to_char.get(id, '<UNK>') for id in token_ids])
    
    @property
    def vocab_size(self):
        return len(self.vocab)

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®"""
    sample_text = """
    Once upon a time, in a land far away, there lived a brave knight named Arthur.
    Arthur was known throughout the kingdom for his courage and wisdom.
    One day, a mysterious dragon appeared and threatened the peaceful village.
    The villagers were scared and didn't know what to do.
    Arthur decided to face the dragon and protect his people.
    He took his sword and shield and went to meet the beast.
    After a fierce battle, Arthur defeated the dragon and saved the village.
    The people celebrated and Arthur became a legend.
    From that day forward, peace returned to the land.
    And they all lived happily ever after.
    
    The end of this simple story shows how courage can overcome fear.
    Sometimes we must be brave to protect what we love.
    Arthur's story teaches us about heroism and selflessness.
    Every hero starts as an ordinary person who chooses to do extraordinary things.
    """
    
    return sample_text.strip()

def calculate_perplexity(loss):
    """è®¡ç®—å›°æƒ‘åº¦"""
    return math.exp(loss)

class GPTTrainer:
    """GPT è®­ç»ƒå™¨"""
    
    def __init__(
        self, 
        model: GPTLMHeadModel,
        tokenizer,
        device: str = 'cpu'
    ):
        self.model = model.to(device)
        self.tokenizer = tokenizer
        self.device = device
        
        # è®­ç»ƒå†å²
        self.train_losses = []
        self.train_perplexities = []
        
    def train_epoch(self, dataloader, optimizer, scheduler=None):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        for batch_idx, (input_ids, target_ids) in enumerate(dataloader):
            input_ids = input_ids.to(self.device)
            target_ids = target_ids.to(self.device)
            
            # å‰å‘ä¼ æ’­
            logits, loss, _ = self.model(input_ids, labels=target_ids)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            optimizer.step()
            if scheduler:
                scheduler.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            # æ‰“å°è¿›åº¦
            if batch_idx % 10 == 0:
                perplexity = calculate_perplexity(loss.item())
                print(f'Batch {batch_idx}, Loss: {loss.item():.4f}, Perplexity: {perplexity:.2f}')
        
        avg_loss = total_loss / num_batches
        avg_perplexity = calculate_perplexity(avg_loss)
        
        self.train_losses.append(avg_loss)
        self.train_perplexities.append(avg_perplexity)
        
        return avg_loss, avg_perplexity
    
    def evaluate(self, dataloader):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for input_ids, target_ids in dataloader:
                input_ids = input_ids.to(self.device)
                target_ids = target_ids.to(self.device)
                
                logits, loss, _ = self.model(input_ids, labels=target_ids)
                total_loss += loss.item()
                num_batches += 1
        
        avg_loss = total_loss / num_batches
        avg_perplexity = calculate_perplexity(avg_loss)
        
        return avg_loss, avg_perplexity
    
    def generate_sample(self, prompt: str, max_length: int = 100, temperature: float = 0.8):
        """ç”Ÿæˆæ–‡æœ¬æ ·æœ¬"""
        self.model.eval()
        
        # ç¼–ç æç¤º
        input_ids = torch.tensor([self.tokenizer.encode(prompt)], dtype=torch.long).to(self.device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            generated = self.model.generate(
                input_ids,
                max_length=max_length,
                temperature=temperature,
                do_sample=True,
                top_k=50
            )
        
        # è§£ç 
        generated_text = self.tokenizer.decode(generated[0].cpu().tolist())
        return generated_text
    
    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(self.train_losses)
        ax1.set_title('Training Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.grid(True)
        
        # å›°æƒ‘åº¦æ›²çº¿
        ax2.plot(self.train_perplexities)
        ax2.set_title('Training Perplexity')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Perplexity')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig('gpt/training_curves.png', dpi=300, bbox_inches='tight')
        plt.show()

def create_optimizer_and_scheduler(model, num_training_steps, learning_rate=1e-4):
    """åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    optimizer = optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=0.01,
        betas=(0.9, 0.95)
    )
    
    # Warmup + Cosine Annealing
    warmup_steps = num_training_steps // 10
    
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        else:
            progress = (step - warmup_steps) / (num_training_steps - warmup_steps)
            return 0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * progress))
    
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    return optimizer, scheduler

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹ GPT è®­ç»ƒæ¼”ç¤º")
    print("=" * 50)
    
    # è®¾å¤‡æ£€æŸ¥
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºåˆ†è¯å™¨
    tokenizer = BPETokenizer()
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    
    # åˆ›å»ºæ¨¡å‹
    config = GPTConfig(
        vocab_size=tokenizer.vocab_size,
        n_positions=256,
        n_embd=128,
        n_layer=6,
        n_head=8,
        n_inner=512
    )
    
    model = GPTLMHeadModel(config)
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # å‡†å¤‡æ•°æ®
    print("\nå‡†å¤‡è®­ç»ƒæ•°æ®...")
    text_data = create_sample_data()
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = SimpleTextDataset(text_data, tokenizer, seq_length=64)
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)
    
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = GPTTrainer(model, tokenizer, device)
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    num_epochs = 20
    num_training_steps = len(train_dataloader) * num_epochs
    optimizer, scheduler = create_optimizer_and_scheduler(model, num_training_steps)
    
    # è®­ç»ƒå¾ªç¯
    print("\nå¼€å§‹è®­ç»ƒ...")
    print("=" * 50)
    
    best_val_loss = float('inf')
    
    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        print("-" * 30)
        
        # è®­ç»ƒ
        start_time = time.time()
        train_loss, train_perplexity = trainer.train_epoch(train_dataloader, optimizer, scheduler)
        
        # éªŒè¯
        val_loss, val_perplexity = trainer.evaluate(val_dataloader)
        
        epoch_time = time.time() - start_time
        
        print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå›°æƒ‘åº¦: {train_perplexity:.2f}")
        print(f"éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å›°æƒ‘åº¦: {val_perplexity:.2f}")
        print(f"è€—æ—¶: {epoch_time:.2f}s")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
            }, 'gpt/best_model.pth')
            print("ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
        
        # ç”Ÿæˆæ ·æœ¬
        if (epoch + 1) % 5 == 0:
            print("\nğŸ“ ç”Ÿæˆæ–‡æœ¬æ ·æœ¬:")
            sample = trainer.generate_sample("Once upon a time", max_length=100, temperature=0.8)
            print(f"'{sample}'")
    
    print("\nâœ… è®­ç»ƒå®Œæˆ!")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    print("ğŸ“Š ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
    trainer.plot_training_curves()
    
    # æœ€ç»ˆæµ‹è¯•
    print("\nğŸ§ª æœ€ç»ˆæµ‹è¯•:")
    prompts = [
        "Once upon a time",
        "Arthur was",
        "The dragon",
        "Peace returned"
    ]
    
    for prompt in prompts:
        generated = trainer.generate_sample(prompt, max_length=50, temperature=0.7)
        print(f"Prompt: '{prompt}'")
        print(f"Generated: '{generated}'")
        print("-" * 40)

if __name__ == "__main__":
    main()

# âš¡ æ¨¡åž‹ä¼˜åŒ–ä¸Žéƒ¨ç½²æ•™ç¨‹

> **ä»Žè®­ç»ƒåˆ°ç”Ÿäº§ï¼šå¤§æ¨¡åž‹ä¼˜åŒ–å’Œéƒ¨ç½²çš„å®Œæ•´æŒ‡å—**

## ðŸŽ¯ æ¦‚è¿°

å¤§æ¨¡åž‹çš„ä¼˜åŒ–å’Œéƒ¨ç½²æ˜¯ä»Žç ”ç©¶åˆ°ç”Ÿäº§çš„å…³é”®çŽ¯èŠ‚ï¼Œæ¶‰åŠæ¨¡åž‹åŽ‹ç¼©ã€æŽ¨ç†åŠ é€Ÿã€å†…å­˜ä¼˜åŒ–ç­‰å¤šä¸ªæ–¹é¢ã€‚

## ðŸ“š æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯

### ðŸ—œï¸ æ¨¡åž‹åŽ‹ç¼©æŠ€æœ¯

#### 1. é‡åŒ– (Quantization)
```python
# INT8é‡åŒ–ç¤ºä¾‹
def quantize_model(model):
    # FP32 â†’ INT8ï¼Œå‡å°‘4å€å†…å­˜
    quantized_model = torch.quantization.quantize_dynamic(
        model, 
        {torch.nn.Linear}, 
        dtype=torch.qint8
    )
    return quantized_model
```

**ä¼˜åŠ¿**:
- ðŸ“‰ å†…å­˜å ç”¨å‡å°‘ 50-75%
- âš¡ æŽ¨ç†é€Ÿåº¦æå‡ 2-4x
- ðŸ’° éƒ¨ç½²æˆæœ¬å¤§å¹…é™ä½Ž

**ç±»åž‹å¯¹æ¯”**:
| é‡åŒ–ç±»åž‹ | ç²¾åº¦æŸå¤± | åŽ‹ç¼©æ¯” | é€‚ç”¨åœºæ™¯ |
|----------|----------|--------|----------|
| INT8 | å¾ˆå° | 4:1 | é€šç”¨éƒ¨ç½² |
| INT4 | å° | 8:1 | èµ„æºå—é™ |
| INT2 | ä¸­ç­‰ | 16:1 | æžç«¯åŽ‹ç¼© |

#### 2. å‰ªæž (Pruning)
```python
def prune_model(model, sparsity=0.5):
    # ç§»é™¤ä¸é‡è¦çš„æƒé‡è¿žæŽ¥
    for module in model.modules():
        if isinstance(module, torch.nn.Linear):
            prune.l1_unstructured(module, name='weight', amount=sparsity)
    return model
```

**æ–¹æ³•åˆ†ç±»**:
- **éžç»“æž„åŒ–å‰ªæž**: ç§»é™¤ä¸ªåˆ«æƒé‡
- **ç»“æž„åŒ–å‰ªæž**: ç§»é™¤æ•´ä¸ªç¥žç»å…ƒ/é€šé“
- **åŠ¨æ€å‰ªæž**: è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”å‰ªæž

#### 3. çŸ¥è¯†è’¸é¦ (Knowledge Distillation)
```python
def distillation_loss(student_logits, teacher_logits, temperature=3.0):
    # å­¦ç”Ÿæ¨¡åž‹å­¦ä¹ æ•™å¸ˆæ¨¡åž‹çš„"è½¯"çŸ¥è¯†
    soft_targets = F.softmax(teacher_logits / temperature, dim=-1)
    soft_prob = F.log_softmax(student_logits / temperature, dim=-1)
    return F.kl_div(soft_prob, soft_targets, reduction='batchmean')
```

**åº”ç”¨åœºæ™¯**:
- ðŸ« å¤§æ¨¡åž‹ â†’ å°æ¨¡åž‹: GPT-3.5 â†’ DistilBERT
- ðŸŽ¯ ä»»åŠ¡ä¸“ç”¨: é€šç”¨æ¨¡åž‹ â†’ é¢†åŸŸæ¨¡åž‹
- ðŸ“± ç§»åŠ¨ç«¯éƒ¨ç½²: æœåŠ¡å™¨æ¨¡åž‹ â†’ æ‰‹æœºæ¨¡åž‹

### ðŸš€ æŽ¨ç†ä¼˜åŒ–æŠ€æœ¯

#### 1. ç®—å­èžåˆ (Operator Fusion)
```python
# å°†å¤šä¸ªæ“ä½œåˆå¹¶ä¸ºå•ä¸ªå†…æ ¸
# ä¾‹å¦‚: LayerNorm + Linear â†’ FusedLayerNormLinear
def fused_layer_norm_linear(x, weight, bias, norm_weight, norm_bias):
    # ä¸€æ¬¡æ€§å®Œæˆå½’ä¸€åŒ–å’Œçº¿æ€§å˜æ¢
    normalized = layer_norm(x, norm_weight, norm_bias)
    return F.linear(normalized, weight, bias)
```

#### 2. åŠ¨æ€æ‰¹å¤„ç† (Dynamic Batching)
```python
class DynamicBatcher:
    def __init__(self, max_batch_size=32, max_wait_time=50):
        self.max_batch_size = max_batch_size
        self.max_wait_time = max_wait_time
        self.pending_requests = []
    
    async def add_request(self, request):
        self.pending_requests.append(request)
        if len(self.pending_requests) >= self.max_batch_size:
            return await self.process_batch()
        # ç­‰å¾…æ›´å¤šè¯·æ±‚æˆ–è¶…æ—¶
```

#### 3. KVç¼“å­˜ä¼˜åŒ–
```python
class KVCache:
    def __init__(self, max_seq_len, num_heads, head_dim):
        self.cache_k = torch.zeros(max_seq_len, num_heads, head_dim)
        self.cache_v = torch.zeros(max_seq_len, num_heads, head_dim)
        self.seq_len = 0
    
    def append(self, new_k, new_v):
        # å¢žé‡è®¡ç®—ï¼Œé¿å…é‡å¤è®¡ç®—åŽ†å²token
        self.cache_k[self.seq_len] = new_k
        self.cache_v[self.seq_len] = new_v
        self.seq_len += 1
```

### ðŸ—ï¸ æž¶æž„ä¼˜åŒ–

#### 1. æ··åˆç²¾åº¦è®­ç»ƒ (Mixed Precision)
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch in dataloader:
    optimizer.zero_grad()
    
    with autocast():  # FP16å‰å‘ä¼ æ’­
        outputs = model(batch)
        loss = criterion(outputs, targets)
    
    scaler.scale(loss).backward()  # ç¼©æ”¾æ¢¯åº¦
    scaler.step(optimizer)
    scaler.update()
```

**ä¼˜åŠ¿**:
- ðŸ“Š å†…å­˜å ç”¨å‡åŠ
- âš¡ è®­ç»ƒé€Ÿåº¦æå‡ 1.5-2x
- ðŸŽ¯ ä¿æŒæ•°å€¼ç¨³å®šæ€§

#### 2. æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing)
```python
def checkpoint_forward(function, *args):
    # åªä¿å­˜éƒ¨åˆ†æ¿€æ´»å€¼ï¼Œéœ€è¦æ—¶é‡æ–°è®¡ç®—
    return torch.utils.checkpoint.checkpoint(function, *args)

# åœ¨Transformerå±‚ä¸­ä½¿ç”¨
class TransformerLayer(nn.Module):
    def forward(self, x):
        if self.training:
            return checkpoint_forward(self._forward_impl, x)
        else:
            return self._forward_impl(x)
```

#### 3. å¹¶è¡Œç­–ç•¥

##### æ•°æ®å¹¶è¡Œ (Data Parallel)
```python
# å¤šGPUè®­ç»ƒ
model = nn.DataParallel(model)
# æˆ–åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ
model = nn.parallel.DistributedDataParallel(model)
```

##### æ¨¡åž‹å¹¶è¡Œ (Model Parallel)
```python
class ModelParallelTransformer(nn.Module):
    def __init__(self):
        super().__init__()
        # ä¸åŒå±‚æ”¾åœ¨ä¸åŒGPUä¸Š
        self.layer1 = TransformerLayer().to('cuda:0')
        self.layer2 = TransformerLayer().to('cuda:1')
    
    def forward(self, x):
        x = self.layer1(x.to('cuda:0'))
        x = self.layer2(x.to('cuda:1'))
        return x
```

##### æµæ°´çº¿å¹¶è¡Œ (Pipeline Parallel)
```python
# ä½¿ç”¨FairScaleæˆ–DeepSpeed
from fairscale.nn import Pipe

model = Pipe(transformer_layers, balance=[2, 2, 2, 2])
```

## ðŸš€ éƒ¨ç½²æŠ€æœ¯æ ˆ

### 1. æŽ¨ç†å¼•æ“Žå¯¹æ¯”

| å¼•æ“Ž | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ | æ€§èƒ½æå‡ |
|------|------|----------|----------|
| **ONNX Runtime** | è·¨å¹³å°ï¼Œæ˜“é›†æˆ | é€šç”¨éƒ¨ç½² | 2-3x |
| **TensorRT** | NVIDIAä¼˜åŒ– | GPUæŽ¨ç† | 3-5x |
| **OpenVINO** | Intelä¼˜åŒ– | CPUæŽ¨ç† | 2-4x |
| **TVM** | æ·±åº¦ä¼˜åŒ– | å®šåˆ¶ç¡¬ä»¶ | 3-6x |

### 2. æœåŠ¡åŒ–éƒ¨ç½²

#### FastAPI + Uvicorn
```python
from fastapi import FastAPI
import torch

app = FastAPI()
model = load_optimized_model()

@app.post("/generate")
async def generate_text(prompt: str):
    with torch.no_grad():
        response = model.generate(prompt)
    return {"response": response}

# å¯åŠ¨: uvicorn main:app --workers 4
```

#### TorchServe
```bash
# æ¨¡åž‹æ‰“åŒ…
torch-model-archiver --model-name transformer \
    --version 1.0 \
    --serialized-file model.pt \
    --handler custom_handler.py

# éƒ¨ç½²æœåŠ¡
torchserve --start --model-store model_store \
    --models transformer=transformer.mar
```

### 3. äº‘ç«¯éƒ¨ç½²æ–¹æ¡ˆ

#### Kubernetes + GPU
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm-service
  template:
    metadata:
      labels:
        app: llm-service
    spec:
      containers:
      - name: llm-container
        image: llm-service:latest
        resources:
          limits:
            nvidia.com/gpu: 1
          requests:
            memory: "16Gi"
            cpu: "4"
```

#### Auto Scaling
```python
# åŸºäºŽé˜Ÿåˆ—é•¿åº¦çš„è‡ªåŠ¨æ‰©ç¼©å®¹
class AutoScaler:
    def __init__(self, min_replicas=1, max_replicas=10):
        self.min_replicas = min_replicas
        self.max_replicas = max_replicas
    
    def should_scale_up(self, queue_length, avg_response_time):
        return queue_length > 50 or avg_response_time > 2000  # ms
    
    def should_scale_down(self, queue_length, avg_response_time):
        return queue_length < 10 and avg_response_time < 500  # ms
```

## ðŸ“Š æ€§èƒ½ç›‘æŽ§

### 1. å…³é”®æŒ‡æ ‡
- **å»¶è¿Ÿ**: P50, P95, P99å“åº”æ—¶é—´
- **åžåé‡**: QPS (Queries Per Second)
- **èµ„æºåˆ©ç”¨çŽ‡**: GPU/CPU/å†…å­˜ä½¿ç”¨çŽ‡
- **å‡†ç¡®çŽ‡**: æ¨¡åž‹è¾“å‡ºè´¨é‡ç›‘æŽ§

### 2. ç›‘æŽ§å®žçŽ°
```python
import time
import psutil
import GPUtil

class PerformanceMonitor:
    def __init__(self):
        self.request_times = []
        self.gpu_usage = []
    
    def log_request(self, start_time, end_time):
        latency = (end_time - start_time) * 1000  # ms
        self.request_times.append(latency)
    
    def log_gpu_usage(self):
        gpus = GPUtil.getGPUs()
        if gpus:
            self.gpu_usage.append(gpus[0].load * 100)
    
    def get_metrics(self):
        return {
            'avg_latency': np.mean(self.request_times),
            'p95_latency': np.percentile(self.request_times, 95),
            'avg_gpu_usage': np.mean(self.gpu_usage)
        }
```

## ðŸ”§ å®žè·µå»ºè®®

### 1. ä¼˜åŒ–æµç¨‹
```
åŸºçº¿æµ‹è¯• â†’ ç“¶é¢ˆåˆ†æž â†’ é’ˆå¯¹æ€§ä¼˜åŒ– â†’ æ•ˆæžœéªŒè¯ â†’ æŒç»­ç›‘æŽ§
```

### 2. ä¼˜åŒ–ä¼˜å…ˆçº§
1. **é‡åŒ–**: æœ€å®¹æ˜“å®žçŽ°ï¼Œæ•ˆæžœæ˜¾è‘—
2. **ç®—å­èžåˆ**: ä¸­ç­‰éš¾åº¦ï¼Œç¨³å®šæ”¶ç›Š
3. **æ¨¡åž‹å‰ªæž**: éœ€è¦é‡è®­ç»ƒï¼Œé•¿æœŸæ”¶ç›Š
4. **çŸ¥è¯†è’¸é¦**: é«˜æŠ•å…¥ï¼Œé«˜å›žæŠ¥

### 3. éƒ¨ç½²æ£€æŸ¥æ¸…å•
- âœ… æ¨¡åž‹æ ¼å¼è½¬æ¢ (PyTorch â†’ ONNX/TensorRT)
- âœ… æŽ¨ç†å¼•æ“Žé€‰æ‹©å’Œé…ç½®
- âœ… æ‰¹å¤„ç†ç­–ç•¥è®¾è®¡
- âœ… ç¼“å­˜æœºåˆ¶å®žçŽ°
- âœ… ç›‘æŽ§å’Œå‘Šè­¦è®¾ç½®
- âœ… è´Ÿè½½æµ‹è¯•å’Œå®¹é‡è§„åˆ’

## ðŸŒŸ æœªæ¥è¶‹åŠ¿

### æŠ€æœ¯å‘å±•
- **ç¡¬ä»¶ä¸“ç”¨åŒ–**: AIèŠ¯ç‰‡ã€ç¥žç»ç½‘ç»œå¤„ç†å™¨
- **è½¯ç¡¬ä»¶ååŒ**: ç¼–è¯‘å™¨ä¼˜åŒ–ã€ç¡¬ä»¶æ„ŸçŸ¥è®­ç»ƒ
- **è¾¹ç¼˜éƒ¨ç½²**: ç§»åŠ¨ç«¯ã€IoTè®¾å¤‡ä¼˜åŒ–
- **ç»¿è‰²AI**: èƒ½è€—ä¼˜åŒ–ã€ç¢³è¶³è¿¹å‡å°‘

### å·¥ç¨‹åŒ–æˆç†Ÿåº¦
- **è‡ªåŠ¨åŒ–å·¥å…·**: ä¸€é”®ä¼˜åŒ–å’Œéƒ¨ç½²
- **æ ‡å‡†åŒ–æµç¨‹**: MLOpsæœ€ä½³å®žè·µ
- **ç”Ÿæ€å®Œå–„**: ä¸°å¯Œçš„å¼€æºå·¥å…·é“¾

æŽŒæ¡è¿™äº›ä¼˜åŒ–å’Œéƒ¨ç½²æŠ€æœ¯ï¼Œæ˜¯å°†AIç ”ç©¶æˆæžœè½¬åŒ–ä¸ºå®žé™…äº§å“çš„å…³é”®ï¼

"""
ç›‘ç£å¾®è°ƒ (Supervised Fine-tuning, SFT) è®­ç»ƒå™¨
å®ç°æŒ‡ä»¤-å›ç­”å¯¹çš„ç›‘ç£å­¦ä¹ è®­ç»ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import math
import time
from typing import List, Dict, Optional, Tuple
from pathlib import Path

# å‡è®¾æˆ‘ä»¬ä½¿ç”¨ä¹‹å‰å®ç°çš„ GPT æ¨¡å‹ä½œä¸ºåŸºç¡€
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'gpt'))

try:
    from gpt_model import GPTLMHeadModel, GPTConfig
except ImportError:
    print("æ³¨æ„ï¼šæ— æ³•å¯¼å…¥ GPT æ¨¡å‹ï¼Œå°†ä½¿ç”¨ç®€åŒ–å®ç°")

class InstructionDataset(Dataset):
    """
    æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†
    
    å¤„ç†æŒ‡ä»¤-å›ç­”å¯¹æ•°æ®ï¼Œæ”¯æŒå¤šç§æ ¼å¼
    """
    
    def __init__(
        self, 
        data_path: str, 
        tokenizer, 
        max_length: int = 512,
        instruction_template: str = "### Instruction:\n{instruction}\n\n### Response:\n{response}"
    ):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.instruction_template = instruction_template
        
        # åŠ è½½æ•°æ®
        self.data = self.load_data(data_path)
        
        print(f"åŠ è½½äº† {len(self.data)} æ¡æŒ‡ä»¤æ•°æ®")
    
    def load_data(self, data_path: str) -> List[Dict]:
        """åŠ è½½æŒ‡ä»¤æ•°æ®"""
        if data_path.endswith('.json'):
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            # å¦‚æœæ²¡æœ‰çœŸå®æ•°æ®æ–‡ä»¶ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®
            data = self.create_sample_data()
        
        return data
    
    def create_sample_data(self) -> List[Dict]:
        """åˆ›å»ºç¤ºä¾‹æŒ‡ä»¤æ•°æ®"""
        sample_data = [
            {
                "instruction": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
                "response": "äººå·¥æ™ºèƒ½(AI)æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚è¿™åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€æ„ŸçŸ¥ã€ç†è§£è‡ªç„¶è¯­è¨€ã€è¯†åˆ«æ¨¡å¼ç­‰èƒ½åŠ›ã€‚"
            },
            {
                "instruction": "ç”¨ç®€å•çš„è¯è§£é‡Šæœºå™¨å­¦ä¹ ",
                "response": "æœºå™¨å­¦ä¹ æ˜¯è®©è®¡ç®—æœºä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ è§„å¾‹çš„æŠ€æœ¯ã€‚å°±åƒäººç±»é€šè¿‡ç»éªŒå­¦ä¹ ä¸€æ ·ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡åˆ†æå¤§é‡æ•°æ®æ¥å‘ç°æ¨¡å¼ï¼Œå¹¶ç”¨è¿™äº›æ¨¡å¼æ¥å¯¹æ–°æ•°æ®åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚"
            },
            {
                "instruction": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—",
                "response": "æ˜¥é£è½»æ‹‚ç»¿æŸ³æ¢¢ï¼Œ\nèŠ±å¼€æ»¡æ ‘ç«å¦–å¨†ã€‚\nè¶èˆèœ‚é£å¿™é‡‡èœœï¼Œ\nå¤§åœ°é‡ç°å¥½æ˜¥æœã€‚"
            },
            {
                "instruction": "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
                "response": "å­¦ä¹ ç¼–ç¨‹çš„å»ºè®®æ­¥éª¤ï¼š1. é€‰æ‹©ä¸€é—¨é€‚åˆåˆå­¦è€…çš„è¯­è¨€ï¼Œå¦‚Pythonï¼›2. æŒæ¡åŸºç¡€è¯­æ³•å’Œæ¦‚å¿µï¼›3. é€šè¿‡å®é™…é¡¹ç›®ç»ƒä¹ ï¼›4. é˜…è¯»ä»–äººçš„ä»£ç å­¦ä¹ ï¼›5. æŒç»­ç»ƒä¹ å’Œè§£å†³é—®é¢˜ï¼›6. åŠ å…¥ç¼–ç¨‹ç¤¾åŒºäº¤æµå­¦ä¹ ã€‚"
            },
            {
                "instruction": "è§£é‡Šæ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„åŒºåˆ«",
                "response": "ä¸»è¦åŒºåˆ«ï¼š1. ç‰¹å¾æå–ï¼šæ·±åº¦å­¦ä¹ è‡ªåŠ¨å­¦ä¹ ç‰¹å¾ï¼Œä¼ ç»ŸMLéœ€è¦æ‰‹å·¥è®¾è®¡ï¼›2. æ•°æ®éœ€æ±‚ï¼šæ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡æ•°æ®ï¼Œä¼ ç»ŸMLåœ¨å°æ•°æ®ä¸Šä¹Ÿèƒ½å·¥ä½œï¼›3. è®¡ç®—èµ„æºï¼šæ·±åº¦å­¦ä¹ éœ€è¦æ›´å¤šè®¡ç®—åŠ›ï¼›4. å¯è§£é‡Šæ€§ï¼šä¼ ç»ŸMLæ›´å®¹æ˜“è§£é‡Šï¼Œæ·±åº¦å­¦ä¹ è¾ƒä¸ºé»‘ç›’ï¼›5. é€‚ç”¨åœºæ™¯ï¼šæ·±åº¦å­¦ä¹ åœ¨å›¾åƒã€è¯­éŸ³ã€NLPç­‰å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ã€‚"
            }
        ]
        
        # æ‰©å±•æ•°æ®é›†
        extended_data = []
        for _ in range(20):  # å¤åˆ¶æ•°æ®ä»¥å¢åŠ è®­ç»ƒæ ·æœ¬
            extended_data.extend(sample_data)
        
        return extended_data
    
    def format_instruction(self, instruction: str, response: str) -> str:
        """æ ¼å¼åŒ–æŒ‡ä»¤-å›ç­”å¯¹"""
        return self.instruction_template.format(
            instruction=instruction,
            response=response
        )
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        item = self.data[idx]
        
        # æ ¼å¼åŒ–æ–‡æœ¬
        formatted_text = self.format_instruction(
            item["instruction"], 
            item["response"]
        )
        
        # åˆ†è¯
        if hasattr(self.tokenizer, 'encode'):
            tokens = self.tokenizer.encode(formatted_text)
        else:
            # ç®€å•çš„å­—ç¬¦çº§åˆ†è¯
            tokens = [ord(c) % 256 for c in formatted_text]
        
        # æˆªæ–­æˆ–å¡«å……åˆ°å›ºå®šé•¿åº¦
        if len(tokens) > self.max_length:
            tokens = tokens[:self.max_length]
        else:
            tokens.extend([0] * (self.max_length - len(tokens)))
        
        # åˆ›å»ºæ ‡ç­¾ï¼ˆå‘å³åç§»ä¸€ä½ï¼‰
        input_ids = tokens[:-1]
        labels = tokens[1:]
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'labels': torch.tensor(labels, dtype=torch.long),
            'instruction': item["instruction"],
            'response': item["response"]
        }

class SimpleTokenizer:
    """ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨"""
    
    def __init__(self):
        # æ”¯æŒçš„å­—ç¬¦é›†
        self.chars = []
        # æ·»åŠ åŸºæœ¬ASCIIå­—ç¬¦
        for i in range(256):
            self.chars.append(chr(i))
        
        self.char_to_id = {char: i for i, char in enumerate(self.chars)}
        self.id_to_char = {i: char for i, char in enumerate(self.chars)}
    
    def encode(self, text: str) -> List[int]:
        """ç¼–ç æ–‡æœ¬"""
        return [self.char_to_id.get(c, 0) for c in text]
    
    def decode(self, token_ids: List[int]) -> str:
        """è§£ç token ids"""
        return ''.join([self.id_to_char.get(id, '') for id in token_ids])
    
    @property
    def vocab_size(self):
        return len(self.chars)

class SFTTrainer:
    """ç›‘ç£å¾®è°ƒè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model,
        tokenizer,
        device: str = 'cpu',
        learning_rate: float = 5e-5,
        weight_decay: float = 0.01
    ):
        self.model = model.to(device)
        self.tokenizer = tokenizer
        self.device = device
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay,
            betas=(0.9, 0.95)
        )
        
        # è®­ç»ƒå†å²
        self.train_losses = []
        self.learning_rates = []
        
    def compute_loss(self, batch):
        """è®¡ç®—æŸå¤±"""
        input_ids = batch['input_ids'].to(self.device)
        labels = batch['labels'].to(self.device)
        
        # å‰å‘ä¼ æ’­
        if hasattr(self.model, 'forward'):
            outputs = self.model(input_ids, labels=labels)
            if isinstance(outputs, tuple):
                loss = outputs[1]  # (logits, loss, ...)
            else:
                # æ‰‹åŠ¨è®¡ç®—æŸå¤±
                logits = outputs
                loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
                loss = loss_fct(
                    logits.view(-1, logits.size(-1)),
                    labels.view(-1)
                )
        else:
            # ç®€åŒ–çš„æŸå¤±è®¡ç®—
            loss = torch.tensor(0.5, requires_grad=True)
        
        return loss
    
    def train_epoch(self, dataloader, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = len(dataloader)
        
        print(f"\nğŸ“š Epoch {epoch + 1} å¼€å§‹è®­ç»ƒ")
        print("-" * 50)
        
        for batch_idx, batch in enumerate(dataloader):
            # è®¡ç®—æŸå¤±
            loss = self.compute_loss(batch)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # å‚æ•°æ›´æ–°
            self.optimizer.step()
            
            total_loss += loss.item()
            
            # æ‰“å°è¿›åº¦
            if batch_idx % 5 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f"  Batch {batch_idx:3d}/{num_batches} | "
                      f"Loss: {loss.item():.4f} | "
                      f"LR: {current_lr:.2e}")
        
        avg_loss = total_loss / num_batches
        self.train_losses.append(avg_loss)
        
        print(f"\nğŸ“Š Epoch {epoch + 1} å®Œæˆ:")
        print(f"   å¹³å‡æŸå¤±: {avg_loss:.4f}")
        print(f"   å›°æƒ‘åº¦: {math.exp(avg_loss):.2f}")
        
        return avg_loss
    
    def evaluate(self, dataloader):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in dataloader:
                loss = self.compute_loss(batch)
                total_loss += loss.item()
                num_batches += 1
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        perplexity = math.exp(avg_loss)
        
        return avg_loss, perplexity
    
    def generate_response(self, instruction: str, max_length: int = 100, temperature: float = 0.7):
        """æ ¹æ®æŒ‡ä»¤ç”Ÿæˆå›ç­”"""
        self.model.eval()
        
        # æ ¼å¼åŒ–è¾“å…¥
        prompt = f"### Instruction:\n{instruction}\n\n### Response:\n"
        
        # ç¼–ç 
        input_ids = torch.tensor([self.tokenizer.encode(prompt)], dtype=torch.long).to(self.device)
        
        # ç”Ÿæˆï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
        if hasattr(self.model, 'generate'):
            with torch.no_grad():
                generated = self.model.generate(
                    input_ids,
                    max_length=len(input_ids[0]) + max_length,
                    temperature=temperature,
                    do_sample=True,
                    top_k=50
                )
            
            # è§£ç 
            generated_text = self.tokenizer.decode(generated[0].cpu().tolist())
            
            # æå–å›ç­”éƒ¨åˆ†
            if "### Response:\n" in generated_text:
                response = generated_text.split("### Response:\n")[-1]
            else:
                response = generated_text[len(prompt):]
            
            return response.strip()
        else:
            return "æ¨¡å‹ä¸æ”¯æŒç”ŸæˆåŠŸèƒ½"
    
    def save_model(self, save_path: str):
        """ä¿å­˜æ¨¡å‹"""
        save_dir = Path(save_path).parent
        save_dir.mkdir(parents=True, exist_ok=True)
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_losses': self.train_losses,
        }, save_path)
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    def load_model(self, load_path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(load_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.train_losses = checkpoint.get('train_losses', [])
        
        print(f"ğŸ“‚ æ¨¡å‹å·²ä» {load_path} åŠ è½½")

def create_sample_model(vocab_size: int = 256):
    """åˆ›å»ºç¤ºä¾‹æ¨¡å‹ç”¨äºæ¼”ç¤º"""
    try:
        # å°è¯•ä½¿ç”¨ GPT æ¨¡å‹
        config = GPTConfig(
            vocab_size=vocab_size,
            n_positions=512,
            n_embd=256,
            n_layer=6,
            n_head=8,
            n_inner=1024
        )
        model = GPTLMHeadModel(config)
        print(f"âœ… ä½¿ç”¨ GPT æ¨¡å‹ ({sum(p.numel() for p in model.parameters()):,} å‚æ•°)")
        return model
    except:
        # ç®€åŒ–çš„æ¨¡å‹å®ç°
        class SimpleModel(nn.Module):
            def __init__(self, vocab_size, embed_dim=256):
                super().__init__()
                self.embedding = nn.Embedding(vocab_size, embed_dim)
                self.transformer = nn.TransformerDecoderLayer(
                    d_model=embed_dim,
                    nhead=8,
                    batch_first=True
                )
                self.lm_head = nn.Linear(embed_dim, vocab_size)
                
            def forward(self, input_ids, labels=None):
                x = self.embedding(input_ids)
                x = self.transformer(x, x)  # ç®€åŒ–ç‰ˆæœ¬
                logits = self.lm_head(x)
                
                loss = None
                if labels is not None:
                    loss_fct = nn.CrossEntropyLoss()
                    loss = loss_fct(
                        logits.view(-1, logits.size(-1)),
                        labels.view(-1)
                    )
                
                return logits, loss
        
        model = SimpleModel(vocab_size)
        param_count = sum(p.numel() for p in model.parameters())
        print(f"âœ… ä½¿ç”¨ç®€åŒ–æ¨¡å‹ ({param_count:,} å‚æ•°)")
        return model

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç›‘ç£å¾®è°ƒæµç¨‹"""
    print("ğŸ¯ ç›‘ç£å¾®è°ƒ (SFT) è®­ç»ƒæ¼”ç¤º")
    print("=" * 50)
    
    # è®¾å¤‡è®¾ç½®
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºåˆ†è¯å™¨
    tokenizer = SimpleTokenizer()
    print(f"ğŸ“ è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_sample_model(tokenizer.vocab_size)
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = InstructionDataset(
        data_path="dummy_path",
        tokenizer=tokenizer,
        max_length=256
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset,
        batch_size=4,
        shuffle=True,
        num_workers=0
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        device=device,
        learning_rate=1e-4
    )
    
    # è®­ç»ƒæ¼”ç¤º
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {len(dataset)} ä¸ªæ ·æœ¬...")
    
    num_epochs = 3
    for epoch in range(num_epochs):
        avg_loss = trainer.train_epoch(dataloader, epoch)
        
        # æ¯ä¸ªepochåç”Ÿæˆç¤ºä¾‹
        if epoch % 1 == 0:
            print(f"\nğŸ¤– ç¬¬ {epoch + 1} è½®è®­ç»ƒåçš„ç”Ÿæˆç¤ºä¾‹:")
            test_instruction = "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ "
            response = trainer.generate_response(test_instruction, max_length=50)
            print(f"   æŒ‡ä»¤: {test_instruction}")
            print(f"   å›ç­”: {response}")
    
    # ä¿å­˜æ¨¡å‹
    save_path = "instruction_tuning/sft_model.pth"
    trainer.save_model(save_path)
    
    print(f"\nâœ… ç›‘ç£å¾®è°ƒè®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ˆ è®­ç»ƒæŸå¤±å˜åŒ–: {trainer.train_losses}")

if __name__ == "__main__":
    main()

"""
ğŸ¯ æŒ‡ä»¤å¾®è°ƒ (Instruction Tuning) å¿«é€Ÿå…¥é—¨
æ¼”ç¤ºç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹
"""

import torch
import sys
import os
from pathlib import Path

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥å…¶ä»–æ¨¡å—
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent.parent))

from sft_trainer import SFTTrainer, InstructionDataset, SimpleTokenizer, create_sample_model

def print_banner():
    """æ‰“å°æ¬¢è¿æ¨ªå¹…"""
    banner = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                    ğŸ¯ æŒ‡ä»¤å¾®è°ƒå¿«é€Ÿå…¥é—¨                        â•‘
    â•‘               Instruction Tuning Quick Start                 â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  ğŸ“ SFT    ğŸ† å¥–åŠ±æ¨¡å‹    ğŸ”„ RLHF    ğŸ’¬ å¯¹è¯ä¼˜åŒ–           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)

def explain_instruction_tuning():
    """è§£é‡ŠæŒ‡ä»¤å¾®è°ƒçš„æ¦‚å¿µ"""
    print("""
ğŸ“ æŒ‡ä»¤å¾®è°ƒ (Instruction Tuning) åŸºç¡€çŸ¥è¯†:

ğŸ“š æ ¸å¿ƒæ¦‚å¿µ:
   æŒ‡ä»¤å¾®è°ƒæ˜¯è®©é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œéµå¾ªäººç±»æŒ‡ä»¤çš„å…³é”®æŠ€æœ¯ã€‚
   å®ƒå°†é€šç”¨çš„è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºèƒ½å¤Ÿæ‰§è¡Œç‰¹å®šä»»åŠ¡çš„åŠ©æ‰‹ã€‚

ğŸ”„ æŠ€æœ¯æµç¨‹:
   1. ğŸ“– é¢„è®­ç»ƒæ¨¡å‹ â†’ åŸºç¡€è¯­è¨€ç†è§£èƒ½åŠ›
   2. ğŸ“ ç›‘ç£å¾®è°ƒ (SFT) â†’ æŒ‡ä»¤éµå¾ªèƒ½åŠ›  
   3. ğŸ† å¥–åŠ±æ¨¡å‹è®­ç»ƒ â†’ è´¨é‡è¯„ä¼°èƒ½åŠ›
   4. ğŸ”„ å¼ºåŒ–å­¦ä¹  (RLHF) â†’ äººç±»åå¥½å¯¹é½

ğŸ’¡ å…³é”®æŠ€æœ¯:
   â€¢ SFT: ç”¨æŒ‡ä»¤-å›ç­”å¯¹è®­ç»ƒæ¨¡å‹
   â€¢ RM: è®­ç»ƒå¥–åŠ±æ¨¡å‹è¯„ä¼°å›ç­”è´¨é‡
   â€¢ RLHF: ç”¨äººç±»åé¦ˆä¼˜åŒ–æ¨¡å‹è¡Œä¸º
   â€¢ DPO: ç›´æ¥åå¥½ä¼˜åŒ–ï¼Œç®€åŒ–RLHFæµç¨‹

ğŸŒŸ åº”ç”¨ä»·å€¼:
   â€¢ ğŸ’¬ å¯¹è¯ç³»ç»Ÿ (ChatGPT, Claude)
   â€¢ ğŸ¤– AIåŠ©æ‰‹ (ä»£ç ã€å†™ä½œã€åˆ†æ)
   â€¢ ğŸ“Š ä¸“ä¸šå·¥å…· (åŒ»ç–—ã€æ³•å¾‹ã€æ•™è‚²)
   â€¢ ğŸ¯ å®šåˆ¶åŒ–åº”ç”¨ (ä¼ä¸šä¸“ç”¨åŠ©æ‰‹)
    """)

def demonstrate_instruction_formats():
    """æ¼”ç¤ºä¸åŒçš„æŒ‡ä»¤æ ¼å¼"""
    print("\nğŸ¯ æŒ‡ä»¤æ ¼å¼æ¼”ç¤º:")
    print("=" * 60)
    
    formats = [
        {
            "name": "åŸºç¡€é—®ç­”æ ¼å¼",
            "template": "Q: {instruction}\nA: {response}",
            "example": {
                "instruction": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
                "response": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚æ¨¡å¼ã€‚"
            }
        },
        {
            "name": "æŒ‡ä»¤-å›åº”æ ¼å¼",
            "template": "### Instruction:\n{instruction}\n\n### Response:\n{response}",
            "example": {
                "instruction": "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
                "response": "def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)"
            }
        },
        {
            "name": "å¯¹è¯æ ¼å¼",
            "template": "Human: {instruction}\n\nAssistant: {response}",
            "example": {
                "instruction": "ä½ å¥½ï¼Œèƒ½å¸®æˆ‘è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯Transformerå—ï¼Ÿ",
                "response": "ä½ å¥½ï¼Transformeræ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚"
            }
        }
    ]
    
    for i, fmt in enumerate(formats, 1):
        print(f"\n{i}. {fmt['name']}:")
        print(f"   æ¨¡æ¿: {fmt['template']}")
        print("   ç¤ºä¾‹:")
        formatted = fmt['template'].format(**fmt['example'])
        for line in formatted.split('\n'):
            print(f"      {line}")

def demonstrate_sft_training():
    """æ¼”ç¤ºç›‘ç£å¾®è°ƒè®­ç»ƒè¿‡ç¨‹"""
    print("\nğŸš€ ç›‘ç£å¾®è°ƒ (SFT) è®­ç»ƒæ¼”ç¤º:")
    print("=" * 60)
    
    # è®¾å¤‡è®¾ç½®
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºåˆ†è¯å™¨
    print("\nğŸ“ æ­¥éª¤ 1: å‡†å¤‡åˆ†è¯å™¨")
    tokenizer = SimpleTokenizer()
    print(f"   âœ… åˆ›å»ºå­—ç¬¦çº§åˆ†è¯å™¨ï¼Œè¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ¤– æ­¥éª¤ 2: å‡†å¤‡æ¨¡å‹")
    model = create_sample_model(tokenizer.vocab_size)
    
    # åˆ›å»ºæ•°æ®é›†
    print("\nğŸ“Š æ­¥éª¤ 3: å‡†å¤‡æ•°æ®é›†")
    dataset = InstructionDataset(
        data_path="dummy_path",
        tokenizer=tokenizer,
        max_length=128
    )
    print(f"   âœ… æ•°æ®é›†å¤§å°: {len(dataset)} æ¡æŒ‡ä»¤-å›ç­”å¯¹")
    
    # å±•ç¤ºæ•°æ®æ ·ä¾‹
    print(f"   ğŸ“‹ æ•°æ®æ ·ä¾‹:")
    sample = dataset[0]
    print(f"      æŒ‡ä»¤: {sample['instruction']}")
    print(f"      å›ç­”: {sample['response']}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("\nğŸ¯ æ­¥éª¤ 4: åˆ›å»ºè®­ç»ƒå™¨")
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        device=device,
        learning_rate=1e-4
    )
    print(f"   âœ… è®­ç»ƒå™¨é…ç½®å®Œæˆ")
    print(f"      å­¦ä¹ ç‡: 1e-4")
    print(f"      ä¼˜åŒ–å™¨: AdamW")
    print(f"      æƒé‡è¡°å‡: 0.01")
    
    # å¿«é€Ÿè®­ç»ƒæ¼”ç¤º
    print("\nâš¡ æ­¥éª¤ 5: å¿«é€Ÿè®­ç»ƒæ¼”ç¤º (3ä¸ªepoch)")
    from torch.utils.data import DataLoader
    
    dataloader = DataLoader(
        dataset,
        batch_size=2,
        shuffle=True,
        num_workers=0
    )
    
    num_epochs = 3
    for epoch in range(num_epochs):
        print(f"\nğŸ“š Epoch {epoch + 1}/{num_epochs}")
        avg_loss = trainer.train_epoch(dataloader, epoch)
        
        # ç”Ÿæˆç¤ºä¾‹
        print(f"\nğŸ¤– è®­ç»ƒåç”Ÿæˆæµ‹è¯•:")
        test_instruction = "è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½"
        response = trainer.generate_response(test_instruction, max_length=30)
        print(f"   Q: {test_instruction}")
        print(f"   A: {response}")
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆå¹³å‡æŸå¤±: {avg_loss:.4f}")
    
    return trainer

def demonstrate_inference():
    """æ¼”ç¤ºæ¨ç†å’Œç”Ÿæˆ"""
    print("\nğŸ’¬ æ¨ç†å’Œç”Ÿæˆæ¼”ç¤º:")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå¥½çš„æ¨¡å‹ (è¿™é‡Œä½¿ç”¨ç®€å•æ¼”ç¤º)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    tokenizer = SimpleTokenizer()
    model = create_sample_model(tokenizer.vocab_size)
    
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        device=device
    )
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„æŒ‡ä»¤
    test_instructions = [
        "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
        "å†™ä¸€é¦–å…³äºç§‹å¤©çš„è¯—",
        "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
        "Pythonä¸­å¦‚ä½•å®šä¹‰å‡½æ•°ï¼Ÿ",
        "è¯·æ€»ç»“æ·±åº¦å­¦ä¹ çš„ä¸»è¦ä¼˜ç‚¹"
    ]
    
    print("ğŸ¯ å¤šæ ·åŒ–æŒ‡ä»¤æµ‹è¯•:")
    for i, instruction in enumerate(test_instructions, 1):
        print(f"\n{i}. æŒ‡ä»¤: {instruction}")
        response = trainer.generate_response(instruction, max_length=50)
        print(f"   å›ç­”: {response}")

def show_rlhf_concept():
    """å±•ç¤ºRLHFæ¦‚å¿µ"""
    print("\nğŸ”„ äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹  (RLHF) æ¦‚å¿µ:")
    print("=" * 60)
    
    print("""
ğŸ¯ RLHF æ ¸å¿ƒæ€æƒ³:
   è®©AIæ¨¡å‹å­¦ä¹ äººç±»çš„åå¥½ï¼Œäº§ç”Ÿæ›´ç¬¦åˆäººç±»æœŸæœ›çš„å›ç­”

ğŸ“Š æŠ€æœ¯æµç¨‹:
   1. ğŸ“ SFTé˜¶æ®µ: åŸºç¡€æŒ‡ä»¤éµå¾ªèƒ½åŠ›
      â”œâ”€â”€ è¾“å…¥: æŒ‡ä»¤-å›ç­”å¯¹æ•°æ®
      â”œâ”€â”€ ç›®æ ‡: å­¦ä¼šç†è§£å’Œå›åº”æŒ‡ä»¤
      â””â”€â”€ è¾“å‡º: åˆæ­¥å¯ç”¨çš„æŒ‡ä»¤æ¨¡å‹

   2. ğŸ† å¥–åŠ±æ¨¡å‹(RM)è®­ç»ƒ:
      â”œâ”€â”€ è¾“å…¥: æŒ‡ä»¤ + å¤šä¸ªå€™é€‰å›ç­” + äººç±»åå¥½æ’åº
      â”œâ”€â”€ ç›®æ ‡: å­¦ä¼šè¯„ä¼°å›ç­”è´¨é‡
      â””â”€â”€ è¾“å‡º: èƒ½æ‰“åˆ†çš„å¥–åŠ±æ¨¡å‹

   3. ğŸ”„ PPOå¼ºåŒ–å­¦ä¹ :
      â”œâ”€â”€ è¾“å…¥: æŒ‡ä»¤ + SFTæ¨¡å‹ + å¥–åŠ±æ¨¡å‹
      â”œâ”€â”€ ç›®æ ‡: æœ€å¤§åŒ–å¥–åŠ±åˆ†æ•°
      â””â”€â”€ è¾“å‡º: å¯¹é½äººç±»åå¥½çš„æœ€ç»ˆæ¨¡å‹

ğŸ’¡ å…³é”®æŠ€æœ¯ç‚¹:
   â€¢ ğŸ¯ åå¥½æ•°æ®æ”¶é›†: äººç±»æ ‡æ³¨å‘˜å¯¹å›ç­”è¿›è¡Œæ’åº
   â€¢ ğŸ“Š å¥–åŠ±å»ºæ¨¡: å°†äººç±»åå¥½è½¬åŒ–ä¸ºå¯è®¡ç®—çš„åˆ†æ•°
   â€¢ âš–ï¸ å¹³è¡¡çº¦æŸ: é˜²æ­¢æ¨¡å‹åç¦»åŸå§‹åˆ†å¸ƒå¤ªè¿œ
   â€¢ ğŸ”„ è¿­ä»£ä¼˜åŒ–: æŒç»­æ”¹è¿›æ¨¡å‹è¡¨ç°

ğŸŒŸ åº”ç”¨æ•ˆæœ:
   â€¢ âœ… æ›´æœ‰å¸®åŠ©çš„å›ç­”
   â€¢ âœ… æ›´è¯šå®çš„è¡¨è¾¾
   â€¢ âœ… æ›´å®‰å…¨çš„è¡Œä¸º
   â€¢ âœ… æ›´ç¬¦åˆäººç±»ä»·å€¼è§‚
    """)

def show_modern_techniques():
    """å±•ç¤ºç°ä»£æŒ‡ä»¤å¾®è°ƒæŠ€æœ¯"""
    print("\nğŸš€ ç°ä»£æŒ‡ä»¤å¾®è°ƒæŠ€æœ¯:")
    print("=" * 60)
    
    techniques = [
        {
            "name": "DPO (Direct Preference Optimization)",
            "description": "ç›´æ¥åå¥½ä¼˜åŒ–ï¼Œæ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹",
            "advantages": ["ç®€åŒ–æµç¨‹", "è®­ç»ƒç¨³å®š", "æ•ˆæœæ˜¾è‘—"],
            "use_case": "Claude, Llama2-Chat ç­‰æ¨¡å‹"
        },
        {
            "name": "Constitutional AI",
            "description": "åŸºäºå®ªæ³•åŸåˆ™çš„AIå¯¹é½æ–¹æ³•",
            "advantages": ["ä»·å€¼è§‚å¯¹é½", "è‡ªæˆ‘æ‰¹è¯„", "è¡Œä¸ºè§„èŒƒ"],
            "use_case": "Claude ç³»åˆ—æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯"
        },
        {
            "name": "Self-Instruct",
            "description": "æ¨¡å‹è‡ªæˆ‘ç”ŸæˆæŒ‡ä»¤æ•°æ®è¿›è¡Œè®­ç»ƒ",
            "advantages": ["æ•°æ®æ‰©å……", "é™ä½æˆæœ¬", "å¤šæ ·æ€§æå‡"],
            "use_case": "Stanford Alpaca, Self-Instruct"
        },
        {
            "name": "LoRA Fine-tuning",
            "description": "ä½ç§©é€‚åº”å¾®è°ƒï¼Œé«˜æ•ˆå‚æ•°æ›´æ–°",
            "advantages": ["å‚æ•°æ•ˆç‡", "è®­ç»ƒå¿«é€Ÿ", "èµ„æºèŠ‚çœ"],
            "use_case": "ä¸ªäºº/å°å›¢é˜Ÿå¾®è°ƒå¤§æ¨¡å‹"
        }
    ]
    
    for i, tech in enumerate(techniques, 1):
        print(f"\n{i}. {tech['name']}:")
        print(f"   ğŸ“ æè¿°: {tech['description']}")
        print(f"   âœ¨ ä¼˜åŠ¿: {', '.join(tech['advantages'])}")
        print(f"   ğŸ¯ åº”ç”¨: {tech['use_case']}")

def interactive_demo():
    """äº¤äº’å¼æ¼”ç¤º"""
    print("\nğŸ® äº¤äº’å¼æŒ‡ä»¤å¾®è°ƒæ¼”ç¤º:")
    print("=" * 60)
    
    print("""
ğŸ’¡ è¿™é‡Œæ¼”ç¤ºäº†æŒ‡ä»¤å¾®è°ƒçš„å®Œæ•´æµç¨‹:

ğŸ”§ å¦‚æœæ‚¨æƒ³æ·±å…¥å®è·µ:
   1. å‡†å¤‡é«˜è´¨é‡çš„æŒ‡ä»¤-å›ç­”æ•°æ®
   2. ä½¿ç”¨æ›´å¤§çš„é¢„è®­ç»ƒæ¨¡å‹ (å¦‚ LLaMA, GPT)
   3. é…ç½®æ›´å¼ºçš„è®¡ç®—èµ„æº (GPUé›†ç¾¤)
   4. å®æ–½å®Œæ•´çš„RLHFæµç¨‹
   5. è¿›è¡Œå……åˆ†çš„å®‰å…¨æ€§æµ‹è¯•

ğŸ“š æ¨èå­¦ä¹ èµ„æº:
   â€¢ OpenAI InstructGPT è®ºæ–‡
   â€¢ Anthropic Constitutional AI è®ºæ–‡  
   â€¢ Stanford Alpaca é¡¹ç›®
   â€¢ HuggingFace RLHF æ•™ç¨‹
   â€¢ DeepSpeed Chat æ¡†æ¶
    """)

def main():
    """ä¸»å‡½æ•°"""
    print_banner()
    
    explain_instruction_tuning()
    
    demonstrate_instruction_formats()
    
    print("\n" + "="*80)
    print("ğŸ¯ å¼€å§‹å®é™…æ¼”ç¤º...")
    print("="*80)
    
    try:
        # æ¼”ç¤ºSFTè®­ç»ƒ
        trainer = demonstrate_sft_training()
        
        # æ¼”ç¤ºæ¨ç†
        demonstrate_inference()
        
        # å±•ç¤ºRLHFæ¦‚å¿µ
        show_rlhf_concept()
        
        # å±•ç¤ºç°ä»£æŠ€æœ¯
        show_modern_techniques()
        
        # äº¤äº’å¼æ¼”ç¤º
        interactive_demo()
        
        print(f"\n" + "="*80)
        print("ğŸ‰ æŒ‡ä»¤å¾®è°ƒå¿«é€Ÿå…¥é—¨å®Œæˆï¼")
        print("="*80)
        
        print("""
ğŸ“ æ‚¨å·²ç»å­¦ä¹ äº†:
   âœ… æŒ‡ä»¤å¾®è°ƒçš„åŸºæœ¬æ¦‚å¿µå’Œé‡è¦æ€§
   âœ… SFT (ç›‘ç£å¾®è°ƒ) çš„å®Œæ•´è®­ç»ƒæµç¨‹
   âœ… ä¸åŒæŒ‡ä»¤æ ¼å¼çš„ä½¿ç”¨æ–¹æ³•
   âœ… RLHF äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ åŸç†
   âœ… ç°ä»£æŒ‡ä»¤å¾®è°ƒæŠ€æœ¯å‘å±•è¶‹åŠ¿

ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®:
   1. æ·±å…¥å­¦ä¹  RLHF ç›¸å…³è®ºæ–‡
   2. å®è·µæ›´å¤§è§„æ¨¡çš„æ¨¡å‹å¾®è°ƒ
   3. æ¢ç´¢å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒæŠ€æœ¯
   4. å…³æ³¨AIå®‰å…¨å’Œå¯¹é½ç ”ç©¶
        """)
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ è¿™é€šå¸¸æ˜¯å› ä¸ºæ¨¡å‹è¾ƒç®€å•æˆ–æ•°æ®é‡è¾ƒå°å¯¼è‡´çš„")
        print("   åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¯·ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œæ›´å¤šçš„æ•°æ®")

if __name__ == "__main__":
    main()
